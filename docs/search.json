[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Master thesis",
    "section": "",
    "text": "Preface\nThis book holds the code executed as part of my master thesis for the University Master Degree in Bioinformatics and Computational Biology at the Autonomous University of Madrid.\nPages with code and explanations are organized thematically, rather than chronologically. Note that, because scripts and code were executed in different computers depending on hardware needs, output from execution is not included along its code, just the code utilized. Additionally, many scripts and code were written for the specific file structure within the computers used during the master thesis. So, hardcoded path variables will be present, and as well some scripts that were used to organize files.\nThe master thesis consisted in the analysis of a publicly available scRNA-seq dataset. Particularly, from the following scientific article:\n\n\n\n\n\n\nPaper information\n\n\n\nTitle: scRNA-seq assessment of the human lung, spleen, and esophagus tissue stability after cold preservation\nAuthors (first and last): E. Madisoon, K. B. Meyer et al.\nYear: 2020\nLink: https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1906-x\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome of the code, scripts and their explanations are being held back for confidentiality purposes.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "installations.html",
    "href": "installations.html",
    "title": "1  Installations",
    "section": "",
    "text": "1.1 Operating System\nAll code was executed in computers running Ubuntu 22.04.4 LTS. And therefore making use of default installed tools through the terminal like GNU Bash and the GNU coreutils.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#tools-for-csvtsv-manipulation",
    "href": "installations.html#tools-for-csvtsv-manipulation",
    "title": "1  Installations",
    "section": "1.2 Tools for CSV/TSV manipulation",
    "text": "1.2 Tools for CSV/TSV manipulation\nI installed some tools to work with CSV and TSV files directly in the terminal.\n\nFor CSVs (and sometimes TSVs) I downloaded a binary for csvtk.\nFor TSVs I downloaded the binaries for tsv-utils.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#condamamba",
    "href": "installations.html#condamamba",
    "title": "1  Installations",
    "section": "1.3 Conda/Mamba",
    "text": "1.3 Conda/Mamba\nMost of the software that I used was installed using conda/mamba environments when possible. I’ll write the purpose of the environment as well as the commands that I used to create them (including non conda/mamba commands that aren’t registered by conda/mamba).\n\n\n\n\n\n\nBe careful with conda and R\n\n\n\nNot all R packages are available for installation through the conda channels. So, in some of my R environments, I had to install R packages using the R way. This is not a recommended way to work as conda/mamba cannot keep track of changes made by R to the conda R installation. Once you install something using R, your environment cannot be reproduced alone using conda, you will have to execute the R installations manually. Also, any new installations or updates using conda/mamba can break the environment as they don’t take into account the R changes. Although in my case this didn’t happen, it is still not a recommended way to work.\nWhen you install R in conda, the installation can be found at &lt;path_to_conda_folder&gt;/envs/&lt;environment_name&gt;/lib/R/ (repace the text between angle brackets with your conda system installation and environment name). That is, every package that you install with conda/mamba or directly with R will be inside of this folder. Even if conda doesn’t see some packages because they were installed using R.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#sec-conda-envs",
    "href": "installations.html#sec-conda-envs",
    "title": "1  Installations",
    "section": "1.4 Conda/Mamba environment setup",
    "text": "1.4 Conda/Mamba environment setup\nConfiguring the miniconda3/mamba installation to have more channels and different priorities.\n\nmamba config --add channels bioconda\nmamba config --add channels conda-forge\n\nView the configured channels in conda/mamba:\n\nconda config --show channels",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#sec-d-rstudio",
    "href": "installations.html#sec-d-rstudio",
    "title": "1  Installations",
    "section": "1.5 d_rstudio",
    "text": "1.5 d_rstudio\nPurpose: to have a functional RStudio installation along with the packages for the kallisto-bustools velocity workflow.\nCreation:\nI run the following commands to install RStudio along with the packages that I will need for scRNA data analysis.\nmamba create -n d_rstudio -c conda-forge rstudio-desktop jupyter r-seurat\n\nconda activate d_rstudio\n\nmamba install r-devtools r-tidyverse r-zeallot r-ggally bioconductor-bsgenome.mmusculus.ucsc.mm10 bioconductor-dropletutils bioconductor-annotationhub bioconductor-singler\n\n# Install packages from source\n\nR\n\n# Hard-code the commit for reproducibility\ndevtools::install_github(\"BUStools/BUSpaRse@5b23c9b609ea20259110eb2592720a6019751a90\")  # this can be installed through conda-forge\n\n# Hard-code the commit for reproducibility\ndevtools::install_github(\"satijalab/seurat-wrappers@73466e361ee759c6b1add58faa3bc4e7a2ee5753\")\n\nq()\n\n# Posterior installations\nmamba install r-velocyto.r\n\nmamba install bioconductor-bsgenome.hsapiens.ucsc.hg38\n\nmamba install bioconductor-biomart\nLater I removed unused packages:\nmamba remove bioconductor-bsgenome.hsapiens.ucsc.hg38 bioconductor-bsgenome.mmusculus.ucsc.mm10\nInstalling a package for clustering with Seurat:\nmamba install leidenalg\nmamba install numpy pandas\nInstalling packages to convert to H5AD data\nR\n\n# Hard-code commit for future reproducibility. Skip updates when asked\ndevtools::install_github(\"mojaveazure/seurat-disk@877d4e18ab38c686f5db54f8cd290274ccdbe295\")\nPackage for command line argument parsing.\nmamba install -c conda-forge r-optparse\nPackage for hyperparameter optimization.\nmamba install -c conda-forge r-clustree\nPackage for trajectory inference\nmamba install -c bioconda bioconductor-slingshot\nTool for outputting svgs.\nmamba install -c conda-forge r-svglite\nInstall package sceasy for scRNA-seq format conversion (Seurat - AnnData mainly). And install loompy because it’s required by sceasy for conversion to/from loom format.\nmamba install -c bioconda r-sceasy\nmamba install -c conda-forge loompy\nInstall package MuDataSeurat for Seurat - MuData/AnnData format conversion.\nR\n\n# Skip updates when asked\nremotes::install_github(\"pmbio/MuDataSeurat@e34e9082a3da029e6b21f7a93a193a2cc0d69245\")\nInstalling the jupyter bash kernel for Quarto documents.\nmamba install -c conda-forge bash_kernel\nInstalling “plotly” package for Python and the “kaleido” package to save plots generated with Plotly. And also the deprecated “orca” package and its dependency “processx”\nmamba install -c conda-forge plotly python-kaleido\nmamba install -c plotly plotly-orca\nmamba install -c conda-forge r-processx\nInstall maftools for variant analysis.\nmamba install -c bioconda bioconductor-maftools\nInstall the pals package to have more color palettes available for plotting.\nmamba install -c conda-forge r-pals\nInstall BSgenome GRCh38 reference genome.\nmamba install -c bioconda bioconductor-bsgenome.hsapiens.ucsc.hg38\nInstall NMF package for maftools muation signature analysis.\nmamba install -c conda-forge r-nmf\nInstall packages to plot venn diagrams.\nmamba install -c conda-forge r-ggvenn r-ggvenndiagram r-venn r-venndiagram",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#sec-d-sratools",
    "href": "installations.html#sec-d-sratools",
    "title": "1  Installations",
    "section": "1.6 d_sratools",
    "text": "1.6 d_sratools\nPurpose: to have SRA tools isolated in an environment to be able to download files from SRA\nCreation:\nmamba create -n d_sratools sra-tools",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#sec-d-samtools",
    "href": "installations.html#sec-d-samtools",
    "title": "1  Installations",
    "section": "1.7 d_samtools",
    "text": "1.7 d_samtools\nPurpose: to have samtools, bcftools and related htslib tools isolated in an environment.\nCreation:\nmamba create -n d_samtools -c conda-forge -c bioconda samtools bcftools",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#sec-d-scomatic",
    "href": "installations.html#sec-d-scomatic",
    "title": "1  Installations",
    "section": "1.8 d_scomatic",
    "text": "1.8 d_scomatic\nPurpose: to have an isolated environment with SComatic for scRNA-seq mutation calling\nCreation:\nI create a conda environment for SComatic. I follow the instructions in the tool’s GitHub.\nmamba create -n d_scomatic -c bioconda python=3.7 r-base=3.6.1 samtools datamash bedtools\nI download SComatic from the GitHub repo.\n# A) You can download a zip file with the repository \"Code\" button in the web\n# B) Or you can do the same thing in the linux terminal\n# For future reproducibility\nwget -P /home/dario/bin/ https://github.com/cortes-ciriano-lab/SComatic/archive/f515f4ee3e7c128600215d21992c051c16e0a03f.zip\n# To grab the latest branch\nwget -P /home/dario/bin/ https://github.com/cortes-ciriano-lab/SComatic/archive/main.zip\n\nunzip *zip\nmv SComatic-main SComatic\n\n# You could also clone the repository to keep the files up to date if needed\ngit clone --single-branch https://github.com/cortes-ciriano-lab/SComatic.git /home/dario/bin/\nI install the remaining dependencies as instructed, using the “requirements.txt” file in the repository.\nmamba activate d_scomatic\n\npip install -r requirements.txt",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#sec-d-qc",
    "href": "installations.html#sec-d-qc",
    "title": "1  Installations",
    "section": "1.9 d_qc",
    "text": "1.9 d_qc\nPurpose: to have all the quality control tools for sequencing data analysis (FastQC, MultiQC, RSeQC)\nCreation:\nmamba create -n d_qc fastqc multiqc rseqc\nInstall Qualimap for alignment quality control.\nmamba install -c bioconda qualimap",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#sec-d-scanpy",
    "href": "installations.html#sec-d-scanpy",
    "title": "1  Installations",
    "section": "1.10 d_scanpy",
    "text": "1.10 d_scanpy\nPurpose: scRNA analysis with python. Environment for the scverse tools.\nCreation:\nmamba create -n d_scanpy -c conda-forge scanpy scvelo leidenalg",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#sec-d-igv",
    "href": "installations.html#sec-d-igv",
    "title": "1  Installations",
    "section": "1.11 d_igv",
    "text": "1.11 d_igv\nPurpose: install a genome browser to visualize alignments and other info.\nCreation:\nmamba create -n d_igv igv\n\nmamba install igvtools igv-reports",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#sec-d-vep",
    "href": "installations.html#sec-d-vep",
    "title": "1  Installations",
    "section": "1.12 d_vep",
    "text": "1.12 d_vep\nPurpose: Have a conda environment for the Variant Effect Predictor (VEP) from Ensembl\nCreation:\nI’ll install Ensemble-VEP from a prepared package in Bioconda. This isn’t the official way to install VEP, but it allows ease of installation and reproducibility through conda.\nmamba create -n d_vep -c conda-forge -c bioconda --override-channels ensembl-vep\nI install the cache for the human reference genome (CAUTION: This takes up 26 GiB).\nvep_install -a cf -s homo_sapiens -y GRCh38\nInstall vcf2maf for VCF to MAF conversion (that requires VEP).\nmamba install -c bioconda vcf2maf",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#sec-d-gatk",
    "href": "installations.html#sec-d-gatk",
    "title": "1  Installations",
    "section": "1.13 d_gatk",
    "text": "1.13 d_gatk\nPurpose: Have a conda/mamba environment for the GATK (Genome Analysis Toolkit)\nCreation:\nmamba create -n d_gatk gatk4",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#sec-d-picard",
    "href": "installations.html#sec-d-picard",
    "title": "1  Installations",
    "section": "1.14 d_picard",
    "text": "1.14 d_picard\nPurpose: Have a conda/mamba environment with Picard tools.\nCreation:\nmamba create -n d_picard picard",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#sec-d-bwa",
    "href": "installations.html#sec-d-bwa",
    "title": "1  Installations",
    "section": "1.15 d_bwa",
    "text": "1.15 d_bwa\nPurpose: Have a conda/mamba environment with bwa for short read alignment\nCreation:\nmamba create -n d_bwa -c bioconda bwa\nInstall samtools into the environmente, because it’s needed by bwa.\nmamba install -c bioconda samtools",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "data_download.html",
    "href": "data_download.html",
    "title": "2  Data download",
    "section": "",
    "text": "Important\n\n\n\nBe carefult to check that you have enough storage available before the download, as data can take up a few TBs.\n\n\nData was downloaded from the Human Cell Atlas Data Portal paper page. The files that were of interest were selected, and a Bash command that was generated for download (which is only valid for 7 days) was used to download the files.\ncurl --location --fail 'https://service.azul.data.humancellatlas.org/manifest/files?catalog=dcp31&format=curl&filters=%7B%22projectId%22%3A+%7B%22is%22%3A+%5B%22c4077b3c-5c98-4d26-a614-246d12c2e5d7%22%2C+%22c4077b3c-5c98-4d26-a614-246d12c2e5d7%22%2C+%22c4077b3c-5c98-4d26-a614-246d12c2e5d7%22%5D%7D%2C+%22genusSpecies%22%3A+%7B%22is%22%3A+%5B%22Homo+sapiens%22%5D%7D%2C+%22fileFormat%22%3A+%7B%22is%22%3A+%5B%22fastq.gz%22%5D%7D%7D&objectKey=manifests%2Ff4015291-e300-5bf2-b6b9-e81ddac8c4e6.ca5182c0-4376-57e5-a328-f00b9a3a3252.curlrc' | curl --config -\n\n\n\n\n\n\nCaution\n\n\n\nOn the 17/03/2024 the HCA Data Portal reindexed all of its files. In this process, all UUIDs for the files and bundles have been regenerated and don’t match the previous ones. If you redownload the data, the folders in which the files are downloaded will be called completely different than the ones I downloaded before this date.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data download</span>"
    ]
  },
  {
    "objectID": "metadata_inspection.html",
    "href": "metadata_inspection.html",
    "title": "3  Metadata inspection",
    "section": "",
    "text": "3.1 Generating a summary metadata table\nIn the dowloaded file manifest and in the supplementary tables from the HCA, there is no direct relationship between single cell FASTQs, with BAMs and donor information. Some of the donor information only appears in the 2 tables of the Microsoft Excel Supplementary Table 1 from the paper, but not in the metadata associated with the sequencing files, while file information only appears in the file manifest. I need a table with all samples with donor information and associated sequencing FASTQ and BAM files. For this purpose, I’m going to join information in the file manifest and supplementary tables from the HCA to generate a summary table.\nTo devise the information joining strategy, I looked at each column from each table and noted the columns with information that was relevant. Then, I looked for columns with information to act as keys in the joining of tables. The strategy followed to create the summary table is portrayed in the following subsections.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metadata inspection</span>"
    ]
  },
  {
    "objectID": "metadata_inspection.html#sec-generate-summary-metadata-table",
    "href": "metadata_inspection.html#sec-generate-summary-metadata-table",
    "title": "3  Metadata inspection",
    "section": "",
    "text": "3.1.1 Generate a donor metadata table\nI save the 2 tables in the Supplementary Table 1 Excel file, each into one CSV file, using LibreOffice Calc. I end up with two files “table_s1_donor_metadata.csv” and “table_s1_sample_info.csv”. The “sample_info” table has the information ordered by sequencing file name. I want to have a table with one line per sequencing file, so I will join information from the other metadata tables into this file’s structure.\nI join table “donor_metadata” into “sample_info” using the donor ID as key.\n\n# [nuptse]\n\ncsvtk join -f 'patient;ID' table_s1_sample_info.csv table_s1_donor_metadata.csv | \\\ncsvtk rename -f 'patient' -n 'donor' &gt; table_s1_merged.csv\n\n\n\n3.1.2 Generate singlecell FASTQ table\n\n# [nuptse]\n\n# 1) Filter the HCA metadata table for esophagus, single-cell and fastq files (cstk filter2)\n# 2) Subset the relevant columns from HCA metadata table (csvtk cut)\n# 3) Remove the ending part of the file names (csvtk replace)\n# 4) Remove remove the prefix of donors (csvtk replace)\n# 5) Rename columns (csvtk rename)\n# 6) Remove duplicated rows that are left after replacing strings (csvtk uniq)\ncsvtk filter2 -f '$35==\"esophagus\" && $8==\"fastq.gz\" && $25=~\"10x\"' FileManifest.csv | \\\ncsvtk cut -f bundle_uuid,file_name,sample.biomaterial_core.biomaterial_id | \\\ncsvtk replace -f \"file_name\" -p '_.*$' -r '' | \\\n#csvtk replace -f \"donor_organism.biomaterial_core.biomaterial_id\" -p '^CBTM-' -r '' | \\\ncsvtk rename -f 'bundle_uuid,file_name' -n 'bundle_uuid_sc_fastq,file_name_sc_fastq' | \\\ncsvtk uniq -f 1- &gt; esophagus_singlecell_fastq.csv\n\n\n\n3.1.3 Generate singlecell BAM table\n\n# [nuptse]\n\n# 1) Filter the HCA metadata table for esophagus, single-cell and BAM files (cstk filter2)\n# 2) Subset the relevant columns from HCA metadata table (csvtk cut)\n# 3) Remove the ending part of the file names (csvtk replace)\n# 4) Remove remove the prefix of donors (csvtk replace)\n# 5) Rename columns (csvtk rename)\n# 6) Remove duplicated rows, after  (csvtk uniq)\ncsvtk filter2 -f '$35==\"esophagus\" && $8==\"bam\" && $25=~\"10x\"' FileManifest.csv | \\\ncsvtk cut -f bundle_uuid,file_name,sample.biomaterial_core.biomaterial_id | \\\ncsvtk replace -f \"file_name\" -p '_.*$' -r '' | \\\n#csvtk replace -f \"donor_organism.biomaterial_core.biomaterial_id\" -p '^CBTM-' -r '' | \\\ncsvtk rename -f 'bundle_uuid,file_name' -n 'bundle_uuid_sc_bam,file_name_sc_bam' | \\\ncsvtk uniq -f 1- &gt; esophagus_singlecell_bam.csv\n\n\n\n3.1.4 Combine tables into the summary table\nI join all of the tables that I have generated to create the summary table.\n\n# [nuptse]\n\n# Join the single-cell FASTQ and BAM tables. (Both files have the same nº of rows, no left join needed)\ncsvtk join -f 'sample.biomaterial_core.biomaterial_id' esophagus_singlecell_fastq.csv esophagus_singlecell_bam.csv | \\\ncsvtk cut -f -sample.biomaterial_core.biomaterial_id &gt; esophagus_singlecell_fastq_bam.csv\n\n# Join the donor data to the single-cell data. (Inner join to remove all other data from non esophagus donors from table S1)\ncsvtk join -f 'file_name_sc_fastq;sample' esophagus_singlecell_fastq_bam.csv table_s1_merged.csv &gt; summary_table.csv\n\nI reorganize the table columns and rows to have a final summary table.\n\n# [nuptse]\n\n# 1) Order columns to have the order: file metadata, donor ID, donor metadata\n# 2) Sort rows by donor and timepoint\ncsvtk cut -f 1-4,19-,5-18 summary_table_pre3.csv | \\\ncsvtk sort -k donor -k timepoint:u -L timepoint:&lt;(printf \"T0\\n12h\\n24h\\n72h\\n\") &gt; summary_table_wide.csv",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metadata inspection</span>"
    ]
  },
  {
    "objectID": "metadata_inspection.html#subsetting-useful-metadata-tables",
    "href": "metadata_inspection.html#subsetting-useful-metadata-tables",
    "title": "3  Metadata inspection",
    "section": "3.2 Subsetting useful metadata tables",
    "text": "3.2 Subsetting useful metadata tables\n\n3.2.1 donor-bundle-timepoint table\n\n# [nuptse]\n\ncsvtk fold -f donor,bundle_uuid_fastq -v timepoint summary_table_wide.csv &gt; donor_bundle_timepoint_all.csv\n\n# Modify the timepoint names so that all start with \"T\"\nawk 'BEGIN{FS=\",\"}; NR==1{print $0}; NR&gt;1{gsub(\"^T\",\"\",$3); print $1\",\"$2\",T\"$3}' donor_bundle_timepoint_all.csv &gt; donor_bundle_timepoint_modified.csv\n\n# Create a table with the excluded bundles removed\ngrep -v -f ../runs_excluded.fofn donor_bundle_timepoint_modidfied.csv &gt; donor_bundle_timepoint.csv\n\n# Create a table without timepoints 72h\ncsvtk filter2 -f '$3 != \"T72h\"' donor_bundle_timepoint.csv &gt; donor_bundle_timepoint_mod_without72h.csv\n\n\n\n3.2.2 donor-tissue-timepoint table\n\n# [nuptse]\n\ncsvtk fold -f patient,organ -v timepoint table_s1_merged.csv | csvtk pretty | less -S",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metadata inspection</span>"
    ]
  },
  {
    "objectID": "quality_control.html",
    "href": "quality_control.html",
    "title": "4  Quality control",
    "section": "",
    "text": "4.1 Executing QC tools\nI run FastQC for every scRNA-seq file, and then aggregate the results with MultiQC.\nI organize the FastQC output files into folders.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "quality_control.html#executing-qc-tools",
    "href": "quality_control.html#executing-qc-tools",
    "title": "4  Quality control",
    "section": "",
    "text": "cd /mnt/bazar/dario_TFM/2019.12.31_Meyer/esophagus/singlecell/data\n\nconda activate d_qc\n\nfastqc -t 8 $(find . -type f -name \"*.fastq.gz\")\n\nmultiqc .\n\ncd /mnt/bazar/dario_TFM/2019.12.31_Meyer/esophagus/singlecell/data\n\nwhile read -r folder\ndo\n    cd ${folder}\n    mkdir -p fastqc/\n    mv *.zip fastqc/\n    mv *fastqc.html fastqc/\ndone &lt; &lt;(find $(pwd) -type d | tail -n +2)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "quality_control.html#interpreting-the-results",
    "href": "quality_control.html#interpreting-the-results",
    "title": "4  Quality control",
    "section": "4.2 Interpreting the results",
    "text": "4.2 Interpreting the results\nThe aggregated quality control results are available in the quality_control_results folder of the repository.\nRemarks from the results:\n\nThe files for “_R1” hold the UMIs –&gt; all have the same lenght\nThe files for “_R2” hold the sequenced RNA –&gt; there are 8 with 98 bp, 12 with 96 bp and 3 with 75 bp\n\nthis may be a problem because the files should have 98 nt. According to 10x documentation.\n\n3 runs have a very bad per base quality report from FastQC, and will be excluded:\n\nHCATisStab7413620_S1_L001_R2_001\nHCATisStab7413621_S1_L001_R2_001\nHCATisStab7413622_S1_L001_R2_001\n\n\nThe rest of the runs have good reports. Therefore, I will continue their analysis as they are.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "inspection_authors_seurat.html",
    "href": "inspection_authors_seurat.html",
    "title": "5  Inspect original paper analysis",
    "section": "",
    "text": "5.1 Imports and data loading\nlibrary(Seurat)\n\nhca_esoph &lt;- readRDS(\"/mnt/bazar/dario_TFM/2019.12.31_Meyer/data+analysis/esophagus/singlecell/paper_analysis/oesophagus_ts.rds\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inspect original paper analysis</span>"
    ]
  },
  {
    "objectID": "inspection_authors_seurat.html#seurat-version",
    "href": "inspection_authors_seurat.html#seurat-version",
    "title": "5  Inspect original paper analysis",
    "section": "5.2 Seurat version",
    "text": "5.2 Seurat version\n\n# Check Object version\nVersion(hca_esoph)\n\nThe Seurat object from the paper was created with Seurat 3.0.1. With version 4.0.0 Seurat changed the definition of Seurat objects to be defined by the package SeuratObject, instead of Seurat directly. So, if a Seurat object is created with versions under 4.0.0, they use the old Seurat Object class definition included in the package Seurat, and above 4.0.0 they follow the definition of the package SeuratObject. For this reason, the Seurat object from the paper has errors when loaded by Seurat 4.3.0, because it’s missing some slots that are defined in the newest versions of the class, but not in version 3.0.1.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inspect original paper analysis</span>"
    ]
  },
  {
    "objectID": "inspection_authors_seurat.html#object-overview",
    "href": "inspection_authors_seurat.html#object-overview",
    "title": "5  Inspect original paper analysis",
    "section": "5.3 Object overview",
    "text": "5.3 Object overview\n\nhca_esoph\n\nThe object contains:\n\nOnly 1 assay called “RNA”\n24245 genes (features)\n87947 barcodes (samples)\nPCA and UMAP calculated",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inspect original paper analysis</span>"
    ]
  },
  {
    "objectID": "inspection_authors_seurat.html#view-the-count-table",
    "href": "inspection_authors_seurat.html#view-the-count-table",
    "title": "5  Inspect original paper analysis",
    "section": "5.4 View the count table",
    "text": "5.4 View the count table\nInspect the count table for the only assay available (RNA).\n\nhca_esoph@assays$RNA@count\n\nThe counts are stored as floating point numbers (numeric), though they should be integers because they are counts. Rounding to integers must be performed whenever the counts are used directly for calculations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inspect original paper analysis</span>"
    ]
  },
  {
    "objectID": "inspection_authors_seurat.html#barcode-and-feature-names",
    "href": "inspection_authors_seurat.html#barcode-and-feature-names",
    "title": "5  Inspect original paper analysis",
    "section": "5.5 Barcode and Feature names",
    "text": "5.5 Barcode and Feature names\nCheck the barcode names in their object, to see how they handle the naming of barcodes for all of the samples merged into an object.\n\nhead(colnames(hca_esoph@assays$RNA))\n\nIt seems that they append the unique name identifier of the FASTQ files to the barcode sequence (both separated by a “-1-” string). I will do the same to match their naming convention and be able to compare objects.\nCheck all of the trailing codes after the barcode sequence.\n\nunique(gsub(\"[ACGT]{16}\", \"\", colnames(hca_esoph)))\n\nAll of the barcodes have the “-1-” text followed by the unique file name of the sample/run. They seem to have analyzed the 23 runs. Including runs ending with 7413620, 7413621 and 7413622 that we excluded from analysis due to their bad base quality.\nCheck the gene (feature) names.\n\nhead(rownames(hca_esoph@assays$RNA))\n\nThey seem to be using HGNC symbols.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inspect original paper analysis</span>"
    ]
  },
  {
    "objectID": "inspection_authors_seurat.html#view-metadata",
    "href": "inspection_authors_seurat.html#view-metadata",
    "title": "5  Inspect original paper analysis",
    "section": "5.6 View Metadata",
    "text": "5.6 View Metadata\n\n5.6.1 Gene metadata\nCheck the RNA assay with the count matrix information.\n\nhead(hca_esoph@assays$RNA@meta.features)\n\nThe meta.feature slot of the RNA assay holds the Ensembl gene IDs mapped to each gene symbol. Normally this slot has the features as rownames and information for each feature/gene in columns. This object has the Ensembl IDs for each sample in a different column. I check if there are differences between IDs of different columns.\n\n# Check that adjacents pairs of columns are the same\nfor (i in 2:dim(hca_esoph@assays$RNA@meta.features)[2]) {\n    print(i)\n    print(summary(hca_esoph@assays$RNA@meta.features[[i-1]] == hca_esoph@assays$RNA@meta.features[[i]]))\n}\n\n# Check if the NA values are the same in all columns\nfor (x in hca_esoph@assays$RNA@meta.features) {\n    print(which(is.na(x)))\n}\n\n# See the names of the genes with missing Ensembl IDs (All columns have the same NAs)\nprint(rownames(hca_esoph)[which(is.na(hca_esoph@assays$RNA@meta.features[[1]]))])\n\nAll of the columns with Ensembl IDs hold the same info.\n\n\n5.6.2 Barcode metadata\nCheck the barcode/cells metadata.\n\ncolnames(hca_esoph@meta.data)\n\nOutput:\n\n[1] “Donor” “Time”\n[3] “donor_time” “organ”\n[5] “patient” “sample”\n[7] “n_genes” “percent_mito”\n[9] “n_counts” “leiden”\n[11] “Celltypes_GenomeBiol_2019” “Celltypes_updated_July_2020”\n\nThere are several columns with barcode metadata. The column “leiden” holds the clustering assignment. Cell types for each barcode are assigned at the columns “Celltypes_GenomeBiol_2019” and “Celltypes_updated_July_2020”. According to the columnns names, the second one seems to be the most recent, so I’ll use this one for analyses.\n\nall(hca_esoph@active.idents == hca_esoph@meta.data$Celltypes_GenomeBiol_2019)\n\nThe active identities set in the object match with the “Celltypes_GenomeBiol_2019” column of the metadata, instead of the “updated” one I’m going to use. If I have to use the identities, I’ll have to replace them with the updated ones.\nThe “n_counts” and “n_genes” columns seem to have the statistics that Seurat normally saves to “nCount” and “nFeature”. This change in name may be because they were calculated by an earlier version of Seurat or because they were generated manually. To be completely sure that these columns hold correct information, I will recalculate the values and compare them. I will use the internal Seurat function Seurat:::CalcN that is used to calculate internally the vectors.\n\n# Round counts to integers because they are floating point numbers\nhca_esoph@assays$RNA@counts@x &lt;- round(hca_esoph@assays$RNA@counts@x)\n\n# Recalculate nCount and nFeature\nesoph_stats &lt;- Seurat:::CalcN(hca_esoph)\n\n# Compare nCount.\nsummary(hca_esoph@meta.data$n_count == unname(esoph_stats$nCount))\n\n# Compare nFeature\nsummary(hca_esoph@meta.data$n_genes == unname(esoph_stats$nFeature))\n\nnCount is the same, but nFeature isn’t. This has to be looked further into.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inspect original paper analysis</span>"
    ]
  },
  {
    "objectID": "inspection_authors_seurat.html#plot-umap",
    "href": "inspection_authors_seurat.html#plot-umap",
    "title": "5  Inspect original paper analysis",
    "section": "5.7 Plot UMAP",
    "text": "5.7 Plot UMAP\nPlot the authors cell annotation over their calculated UMAP embedding. This image should resemble the one in the scientific paper.\n\n# [nuptse]\n\nlibrary(ggplot2)\n\n# Use the 'alphabet' color palette that is integrated in Seurat from the `pals` package\np &lt;- DimPlot(hca_esoph, reduction = \"umap\", group.by = \"Celltypes_updated_July_2020\", cols = 'alphabet', combine=FALSE)\np2 &lt;- p[[1]] + \n    coord_fixed(ratio = 1) + \n    labs(title = \"HCA cell type annotation\")\n\nresults &lt;- \"/mnt/bazar/dario_TFM/2019.12.31_Meyer/data+analysis/esophagus/singlecell/results/extending_paper_analysis/embeddings\"\n\nggsave(plot = p2, filename =  paste0(results, \"/clusters_umap.svg\"), width = 3000, height = 3000, units = 'px', bg = 'white')\nggsave(plot = p2, filename =  paste0(results, \"/clusters_umap.png\"), width = 3000, height = 3000, units = 'px', bg = 'white')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inspect original paper analysis</span>"
    ]
  },
  {
    "objectID": "alignment_starsolo.html",
    "href": "alignment_starsolo.html",
    "title": "6  Aligning scRNA-seq with STARsolo",
    "section": "",
    "text": "6.1 Index the reference genome\nBefore aligning reads, we have to generate an index for the reference genome for STAR to work.\n# [folia]\n\ncd ~/TFM/reference_genomes/\n\nmkdir STAR_index\n\n~/bin/STAR_2.7.11a/Linux_x86_64_static/STAR \\\n    --runMode genomeGenerate \\\n    --genomeDir STAR_index/ \\\n    --genomeFastaFiles Homo_sapiens.GRCh38.dna.primary_assembly.fa \\\n    --sjdbGTFfile Homo_sapiens.GRCh38.110.gtf \\\n    --runThreadN 10 \\\n    |& tee logs/star_index.log",
    "crumbs": [
      "Trajectory inference analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Aligning scRNA-seq with STARsolo</span>"
    ]
  },
  {
    "objectID": "alignment_starsolo.html#align-samples",
    "href": "alignment_starsolo.html#align-samples",
    "title": "6  Aligning scRNA-seq with STARsolo",
    "section": "6.2 Align samples",
    "text": "6.2 Align samples\nI align all samples with using Bash script called “star_align.sh”.\n\n#!/bin/bash\n#\n# AUTHOR: Darío González\n# DESCRIPTION: Shell script to run STAR in the lab server folia.\n\n# Assign arguments to variables\ninput_file=$(realpath $1)\n\n# Change to the working directory in folia\nmkdir -p /home/dario/TFM/2019.12.31_Meyer/STAR_align/\ncd /home/dario/TFM/2019.12.31_Meyer/STAR_align\n\n# Read names of files and process them\nwhile read -r folder\ndo\n    # State the file being processed\n    run=$(basename ${folder})\n    echo -e \"\\n[ $(date +'%Y/%m/%d %T.%3N') ] Processing files in ${run}\\n\"\n\n    # Transfer files from nuptse to folia\n    rsync -avR --progress -hh dario@nuptse:/mnt/bazar/dario_TFM/2019.12.31_Meyer/data+analysis/esophagus/singlecell/data/./${run}/*.fastq.gz .\n\n    # Create a folder for the results\n    cd ${run}\n    #rm -rf STAR_out\n    mkdir -p STAR_out\n\n    # Process files\n    ~/bin/STAR_2.7.11a/Linux_x86_64_static/STAR \\\n        --runMode alignReads \\\n        --genomeDir /home/dario/TFM/reference_genomes/STAR_index/ \\\n        --outFileNamePrefix STAR_out/ \\\n        --readFilesCommand zcat \\\n        --readFilesIn *_R2_*.fastq.gz *_R1_*.fastq.gz \\\n        --soloType CB_UMI_Simple \\\n        --soloCBwhitelist /home/dario/TFM/10x_barcodes/737K-august-2016.txt \\\n        --soloCBstart 1 \\\n        --soloCBlen 16 \\\n        --soloUMIstart 17 \\\n        --soloUMIlen 10 \\\n        --soloStrand Forward \\\n        --soloFeatures Gene GeneFull Velocyto \\\n        --outSAMattributes NH HI nM AS CR UR CB UB GX GN sS sQ sM \\\n        --outSAMtype BAM SortedByCoordinate \\\n        --runThreadN 10\n\n    # Transfer result files back to nuptse\n    rsync -av --progress -hh STAR_out dario@nuptse:/mnt/bazar/dario_TFM/2019.12.31_Meyer/data+analysis/esophagus/singlecell/data/${run}/\n\n    # Clean up\n    cd ..\n    rm -rf ${run}\n    echo -e \"\\n[ $(date +'%Y/%m/%d %T.%3N') ] Finished processing ${run}\"\ndone &lt; ${input_file}\n\n# Remove working directory\nrmdir /home/dario/TFM/2019.12.31_Meyer/STAR_align/\n\nOptions explanation (full explanation in the manual):\n\n--runMode alignReads: run STAR to aling reads (this is the default behaviour)\n--genomeDir: directory where the indexed reference genome is located\n--readFilesIn: input the sequencing files (FASTQs). For scRNA-seq, the file that holds the barcode+UMI sequences goes at the end.\n--readFilesCommand: this specifies a command to decompress the input sequence files.\n--soloType CB_UMI_Simple: specifies that we are mapping reads that come from 10x Chromium or Drop-seq scRNA-seq sequencing technologies\n--soloCBwhitelist: give STAR the file of barcodes used in the sequencing so that it can correct sequencing errors in the barcodes.\n--soloStrand Forward: specify the strandness of the reads. By default it’s forward.\n--soloFearues Velocyto: genomic features for which the UMI counts per Cell Barcode are counted. You can choose from. [Gene, SJ, GeneFull, GeneFull ExonOverIntron, GeneFull Ex50pAS, Velocyto]. More than one option can be specified here.\n--soloCellFilter None: Cell filtering parameters. By default STAR filters similar to what cellranger does.\n--soloMultiMappers: tell STAR what to do about multimapping reads. By default, STAR only keeps unique mapping reads, and discards any multimapping.\n\nYou have to tell STARsolo where the barcode and UMIs are in the sequence file that you pass it. By default it’s configured to get the sequences for 10x 3’ v2. If you use a different scRNA-seq technology from 10x 3’ v2, then you have to specify the barcode and UMI length:\n--soloCBstart 1: cell barcode start base --soloCBlen 16: cell barcode length --soloUMIstart 17: UMI start base --soloUMIlen 10: UMI length\nI run my script.\n\n# [folia]\n\ncd /home/dario/TFM/scripts/\n\n./star_align.sh ../data/runs.fofn |& tee ../logs/star_align.log\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you are low on storage, STAR will fail to generate a BAM file, due to insufficient storage it stops before it can finish and the BAM will be left incomplete. But STAR won’t give any errors or warnings. To know if your BAM files are complete run samtools quickcheck &lt;file&gt;. This will check for any errors in the BAM file format. If you had low storage during STAR’s run, the EOF (End-Of-File) block that should mark the end of the BAM file will be missing because the BAM is incomplete.\n\n# Check BAM integrity\nfor i in $(cat runs.fofn); do samtools quickcheck ${i}/STAR_out/Aligned.sortedByCoord.out.bam && echo \"OK\" || echo \"Fail!\"; done\n\n# Check BAM size\nfor i in $(cat runs.fofn); do du -h ${i}/STAR_out/Aligned.sortedByCoord.out.bam; done\n\n\n\nRemove write permissions for all users to avoid accidental deletions of the alignment files.\n\n# [nuptse]\n\ncd /mnt/bazar/dario_TFM/2019.12.31_Meyer/data+analysis/esophagus/singlecell/data\n\nfor i in $(find -type d -name STAR_out); do chmod -R a-w $i; done",
    "crumbs": [
      "Trajectory inference analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Aligning scRNA-seq with STARsolo</span>"
    ]
  },
  {
    "objectID": "alignment_starsolo.html#checking-read-loss",
    "href": "alignment_starsolo.html#checking-read-loss",
    "title": "6  Aligning scRNA-seq with STARsolo",
    "section": "6.3 Checking read loss",
    "text": "6.3 Checking read loss\nIt would be interesting to know how many reads where recovered from the original raw FASTQs after the alignment. I check the read loss after each processing step that STAR performs using a Bash script.\n\n#!/bin/bash\n#\n# AUTHOR: Darío González\n# DESCRIPTION: Calculate loss of sequencing reads in each step of the scRNA-seq analysis pipeline\n\n# Create function to print statistics for each run\nfunction reads_stats() {\n    \n    FOLDER=$1\n\n    # get read counts for each step of the pipeline\n    original=$(unzip -p fastqc/*_R2_*_fastqc.zip *_R2_001_fastqc/fastqc_data.txt | grep \"Total Sequences\" | awk '{print $3}')\n    corrected=$(grep -E \"yes*\" ${FOLDER}/Solo.out/Barcodes.stats | sed -E 's/^\\s+//g; s/\\s+/\\t/g' | awk 'BEGIN{total=0} {total += $2} END{print total}')\n    unique_aligned=$(grep \"Uniquely mapped reads number\" ${FOLDER}/Log.final.out | sed -E \"s/^\\s+//g; s/\\s{2,}/\\t/g\" | awk 'BEGIN{FS=\"\\t\"} {print $2}')\n    spliced=$(tail -n +4 ${FOLDER}/Solo.out/Velocyto/raw/spliced.mtx | cut -d ' ' -f 3 | awk 'BEGIN{total=0} {total += $1} END{print total}')\n    unspliced=$(tail -n +4 ${FOLDER}/Solo.out/Velocyto/raw/unspliced.mtx | cut -d ' ' -f 3 | awk 'BEGIN{total=0} {total += $1} END{print total}')\n    ambiguous=$(tail -n +4 ${FOLDER}/Solo.out/Velocyto/raw/ambiguous.mtx | cut -d ' ' -f 3 | awk 'BEGIN{total=0} {total += $1} END{print total}')\n    spliced_unspliced=$(($spliced + $unspliced))\n    total=$(($spliced + $unspliced + $ambiguous))\n    \n\n    # Calculate percentages\n    original_pc=100\n    corrected_pc=$(echo \"(${corrected}/${original}) * 100\" | bc -l)\n    unique_aligned_pc=$(echo \"(${unique_aligned}/${original}) * 100\" | bc -l)\n    spliced_pc=$(echo \"(${spliced}/${original}) * 100\" | bc -l)\n    unspliced_pc=$(echo \"(${unspliced}/${original}) * 100\" | bc -l)\n    ambiguous_pc=$(echo \"(${ambiguous}/${original}) * 100\" | bc -l)\n    spliced_unspliced_pc=$(echo \"(${spliced_unspliced}/${original}) * 100\" | bc -l)\n    total_pc=$(echo \"(${total}/${original}) * 100\" | bc -l)\n\n    # Output read counts\n    LC_NUMERIC=en_US.UTF-8  # to have the decimal separator set as a point\n\n    printf \"step\\tnumber_reads\\tpercentage\\n\"\n    printf \"original\\t%s\\t%.2f\\n\" $original $original_pc\n    printf \"corrected\\t%s\\t%.2f\\n\" $corrected $corrected_pc\n    printf \"unique_aligned\\t%s\\t%.2f\\n\" $unique_aligned $unique_aligned_pc\n    printf \"spliced\\t%s\\t%.2f\\n\" $spliced $spliced_pc\n    printf \"unspliced\\t%s\\t%.2f\\n\" $unspliced $unspliced_pc\n    printf \"ambiguous\\t%s\\t%.2f\\n\" $ambiguous $ambiguous_pc\n    printf \"spliced+unspliced\\t%s\\t%.2f\\n\" $spliced_unspliced $spliced_unspliced_pc\n    printf \"spliced+unspliced+ambiguous\\t%s\\t%.2f\\n\" $total $total_pc\n}\n\n# Get folder names\nrun_folders=$1\n\n# Set variables\nFOLDER=STAR_out\n\n# Run stats function for all runs\nwhile read -r run_folder; do\n    cd $run_folder\n    run=$(basename ${run_folder})\n    \n    mkdir -p ${run_folder}/${FOLDER}/read_stats\n    reads_stats $FOLDER &gt; ${run_folder}/${FOLDER}/read_stats/read_stats_STAR.tsv\n\n    cd ..\ndone &lt; ${run_folders}\n\nI run my Bash script to generate read loss statistics for all samples.\n\n./read_stats_STAR.sh ../data/runs.fofn\n\nSummarize read loss statistics using my pre-existing python scripts.\n\npython join_read_loss.py --wd /mnt/bazar/dario_TFM/2019.12.31_Meyer/data+analysis/esophagus/singlecell data/runs.fofn /STAR_out/read_stats/read_stats_STAR.tsv results/read_loss_STAR/read_loss_STAR.tsv\n\npython read_loss_summary_plot.py -o ../results/read_loss_STAR/ -n read_loss_STAR ../results/read_loss_STAR/read_loss_STAR.tsv\n\npython read_loss_plot.py ../data/runs.fofn STAR_out/read_stats/reads_loss_STAR.tsv read_loss_STAR.html",
    "crumbs": [
      "Trajectory inference analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Aligning scRNA-seq with STARsolo</span>"
    ]
  },
  {
    "objectID": "alignment_starsolo.html#check-duplication-levels",
    "href": "alignment_starsolo.html#check-duplication-levels",
    "title": "6  Aligning scRNA-seq with STARsolo",
    "section": "6.4 Check duplication levels",
    "text": "6.4 Check duplication levels\nThere is a steep read loss between the alignment step and the count matrices. I have been told that this is normal, and that it happens because there is a deduplication of repeated UMIs. I’m going to look into the UMI duplication levels of one of my samples to assess this.\n\n# [nuptse]\n\ncd /mnt/bazar/dario_TFM/2019.12.31_Meyer/data+analysis/esophagus/singlecell/data/018f7a89-f540-4845-9d2f-f7bde863a008/STAR_out\n\nmamba activate d_samtools\n\n# Get the Corrected barcodes and UMIs\nsamtools view Aligned.sortedByCoord.out.bam | awk '{gsub(\"CB:Z:\", \"\", $23); gsub(\"UB:Z:\", \"\", $24); print $23, $24}' &gt; CBUB.txt\n\n# Count each combination\nsort CBUB.txt | uniq -c | sed -E 's/^\\s+//g' &gt; CBUB.count.txt\n\n# Count the duplicated reads\nawk 'BEGIN{dup=0} {if($2 != \"-\"){ if($1 &gt; 1){dup += ($1-1)} } } END{print dup}' CBUB.count.txt\n\n# Output: 68882416\n\n182.190.451 - 68.882.416 = 113.308.035 final reads\nIn the count matrix there are 96.015.296 total reads, which is less than what I got. This can be explained because STAR uses the option --soloUMIdedup 1MM_All, that groups UMIs with 1 mismatch between each other, if I had used --soloUMIdedup Exact the number of reads in the count matrix would match mine.",
    "crumbs": [
      "Trajectory inference analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Aligning scRNA-seq with STARsolo</span>"
    ]
  },
  {
    "objectID": "alignment_starsolo.html#inspecting-the-alignment-visually",
    "href": "alignment_starsolo.html#inspecting-the-alignment-visually",
    "title": "6  Aligning scRNA-seq with STARsolo",
    "section": "6.5 Inspecting the alignment visually",
    "text": "6.5 Inspecting the alignment visually\nI index all BAM files from STARsolo to visualize them in IGV.\n\n# [nuptse]\n\nfor i in $(find . -maxdepth 3 -name *bam); do echo $i; samtools index -b -@ 4 $i; done\n\nPrepare reference genome annotation for visualization.\n\n# [nuptse]\n\nigvtools sort Homo_sapiens.GRCh38.110.gtf Homo_sapiens.GRCh38.110.sortedByCoord.gtf\n\nigvtools index Homo_sapiens.GRCh38.110.sortedByCoord.gtf",
    "crumbs": [
      "Trajectory inference analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Aligning scRNA-seq with STARsolo</span>"
    ]
  },
  {
    "objectID": "joining_count_matrices.html",
    "href": "joining_count_matrices.html",
    "title": "7  Joining velocity count matrices in a Seurat Object",
    "section": "",
    "text": "7.1 Merge test\nFirst I check what Seurat does when merging two different objects without previous preparation.\nfind_dup_genes &lt;- function(x, mode = \"count\") {\n    \n    dup_genes_rows &lt;- which(duplicated(rownames(x)) | duplicated(rownames(x), fromLast = TRUE))\n    dup_genes &lt;- rownames(x)[dup_genes_rows]\n    \n    if (mode == \"count\") {\n        dup_genes &lt;- as.data.frame(table(dup_genes))\n    } else if (mode == \"unique\") {\n        dup_genes &lt;- unique(dup_genes)\n    } else if (mode == \"all\") {\n        return(dup_genes)\n    } else {\n        stop(\"Unrecognised mode. Only 'count', 'unique' and 'all' are valid.\\n\")\n    }\n    \n    return(dup_genes)\n}\n# Get directories\ndirectories &lt;- dir(path = \"~/TFM/2019.12.31_Meyer/data\",\n                   pattern = \"raw\",\n                   full.names = TRUE,\n                   recursive = TRUE,\n                   include.dirs = TRUE)\n\n# Merge test for the 2 first samples\nc(spliced, unspliced) %&lt;-% read_velocity_output(spliced_dir = directories[1],\n                                    spliced_name = \"spliced\",\n                                    unspliced_dir = directories[1],\n                                    unspliced_name = \"unspliced\")\n\nbcs_use &lt;- intersect(colnames(spliced), colnames(unspliced))\n\nsf &lt;- spliced[, bcs_use]\nuf &lt;- unspliced[, bcs_use]\nprint(find_dup_genes(sf))\nprint(find_dup_genes(uf))\n\nseu1 &lt;- CreateSeuratObject(sf, assay = \"spliced\")\nseu1[[\"unspliced\"]] &lt;- CreateAssayObject(uf)\n\nseu1\n\nc(spliced, unspliced) %&lt;-% read_velocity_output(spliced_dir = directories[2],\n                                    spliced_name = \"spliced\",\n                                    unspliced_dir = directories[2],\n                                    unspliced_name = \"unspliced\")\n\nbcs_use &lt;- intersect(colnames(spliced), colnames(unspliced))\n\nsf &lt;- spliced[, bcs_use]\nuf &lt;- unspliced[, bcs_use]\nprint(find_dup_genes(sf))\nprint(find_dup_genes(uf))\n\nseu2 &lt;- CreateSeuratObject(sf, assay = \"spliced\")\nseu2[[\"unspliced\"]] &lt;- CreateAssayObject(uf)\n\nseu2\n\n# Merge objects\nseu1y2 &lt;- merge(seu1, seu2)\nWhen Seurat merges the 2 objects, it checks if barcodes collide, and it makes the barcodes unique appending numbers 1 and 2 depending on the object that they come from.\nAdding meta.feature information. The paper Seurat object holds Ensembl gene ID information in the RNA assay. So, if I want to match both objects features/genes to compare them I’ll have to add Ensembl IDs to my merged object.\n# Load Ensembl ID to symbol table\nspliced_ensembl2symbol &lt;- read.table(paste0(directories[1], \"/spliced.genes.map.txt\"), header = FALSE, sep = \"\\t\", col.names = c(\"ensembl_id\", \"symbol\"))\n\n# Adding feature metadata to seu1 and seu2\nseu1@assays$spliced &lt;- AddMetaData(seu1@assays$spliced, metadata = spliced_ensembl2symbol$ensembl_id, col.name = \"ensembl_id\")\nseu2@assays$spliced &lt;- AddMetaData(seu2@assays$spliced, metadata = spliced_ensembl2symbol$ensembl_id, col.name = \"ensembl_id\")\n\n# Renaming barcodes to avoid collisions during merge\nseu1 &lt;- RenameCells(seu1, new.names = paste0(rownames(seu1), \"-1\"))\nseu2 &lt;- RenameCells(seu2, new.names = paste0(colnames(seu2), \"-2\"))\n\n# Merge objects\nseu1y2 &lt;- merge(seu1, seu2)\n\n# Check \nhead(seu1y2@assays$spliced@meta.features)\nThe merge() function from Seurat doesn’t keep the meta.feature information in the merge object. So I’ll have to add it directly to the final merged object.",
    "crumbs": [
      "Trajectory inference analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Joining velocity count matrices in a Seurat Object</span>"
    ]
  },
  {
    "objectID": "joining_count_matrices.html#merge-test",
    "href": "joining_count_matrices.html#merge-test",
    "title": "7  Joining velocity count matrices in a Seurat Object",
    "section": "",
    "text": "7.1.1 Merging all samples/runs\nI create a file with the runs/samples folder names and the file names for 10x samples. This will allow me to get the file that is being processed with their folder name and then append their corresponding file name.\ncd /mnt/bazar/dario_TFM/2019.12.31_Meyer/Metadata/Metadata_combined\n\n# Steps:\n# 1) extract the columns (csvtk cut)\n# 2) keep the rows for 10x experiments (csvtk filter)\n# 3) remove the trailing characters after first underscore (sed)\n# 4) remove duplicate rows (uniq)\ncsvtk cut -f bundle_uuid,file_name,library_preparation_protocol.library_construction_approach Metadata_esophagus_empty_dedup.csv | \\\ncsvtk filter2 --out-tabs -f '$3 =~ \"10x\"' | \\\nsed 's/_.*gz//g' | \\\nuniq &gt; /mnt/bazar/dario_TFM/2019.12.31_Meyer/data+analysis/esophagus/singlecell/bundle_filename_10x.tsv\n\n# Same as previous command but with awk instead of sed\n#csvtk cut -f bundle_uuid,file_name,library_preparation_protocol.library_construction_approach Metadata_esophagus_empty_dedup.csv | \\\n#csvtk filter2 --out-tabs -f '$3 =~ \"10x\"' | \\\n#awk 'BEGIN{FS = \"\\t\"; OFS = \"\\t\"} {gsub(/_.*$/, \"\", $2); print $0}' | \\\n#uniq &gt; /mnt/bazar/dario_TFM/2019.12.31_Meyer/data+analysis/esophagus/singlecell/bundle_filename_10x.tsv\nI write a script to merge count matrices.\n\n# AUTHOR: Darío González\n# DESCRIPTION: Merging all samples into one Seurat object\n\n# IMPORTS\nlibrary(BUSpaRse) # github (devtools)\nlibrary(Seurat)\nlibrary(SeuratWrappers) # github (devtools)\nlibrary(zeallot) # For %&lt;-% that unpacks lists in the Python manner\nlibrary(DropletUtils)\nlibrary(plyr)\nlibrary(tidyverse)\nlibrary(Matrix)\n\n# MAIN\n\n# Get directories\ndirectories &lt;- dir(path = \"~/TFM/2019.12.31_Meyer/data\",\n                   pattern = \"raw\",\n                   full.names = TRUE,\n                   recursive = TRUE,\n                   include.dirs = TRUE)\n\n# Load run uuid to file name\nbundle2filename &lt;- read.table(\"~/TFM/2019.12.31_Meyer/bundle_filename_10x.tsv\", header = TRUE, sep = \"\\t\", quote = \"\")\n\n# Read in the matrices\nread_velocity &lt;- function(dir=getwd(),\n                          spliced_file=\"spliced.mtx\",\n                          unspliced_file=\"unspliced.mtx\",\n                          barcodes=\"barcodes.txt\",\n                          genes=\"genes.txt\",\n                          ...) {\n    spliced &lt;- ReadMtx(mtx = paste0(dir, \"/\", spliced_file),\n                       cells = paste0(dir, \"/\", barcodes),\n                       features = paste0(dir, \"/\", genes),\n                       ...)\n    unspliced &lt;- ReadMtx(mtx = paste0(dir, \"/\", unspliced_file),\n                         cells = paste0(dir, \"/\", barcodes),\n                         features = paste0(dir, \"/\", genes),\n                         ...)\n    return(list(spliced, unspliced))\n}\n\n# Make Seurat object from all individual spliced/unspliced files:\ncounter &lt;- 0\n\nfor (run_dir in directories) {\n    # Update counter\n    counter &lt;- counter + 1\n    \n    # State the file being processed\n    cat(\"\\n\", counter, \"Processing\", run_dir, \"\\n\")\n    \n    # Load the count matrices for spliced and unspliced reads\n    c(spliced, unspliced) %&lt;-%  read_velocity(dir = run_dir,\n                                              spliced_file = \"spliced.mtx\",\n                                              unspliced_file = \"unspliced.mtx\",\n                                              barcodes = \"barcodes.tsv\",\n                                              genes = \"features.tsv\",\n                                              cell.column = 1,\n                                              feature.column = 2,\n                                              mtx.transpose = FALSE)\n\n    # Filter matrices so that barcodes that appear in both remain\n    # sf = spliced filtered, uf = unspliced filtered\n    bcs_use &lt;- intersect(colnames(spliced), colnames(unspliced))\n    sf &lt;- spliced[, bcs_use]\n    uf &lt;- unspliced[, bcs_use]\n    \n    # Create Seurat Object\n    seu &lt;- CreateSeuratObject(sf, assay = \"spliced\")\n    seu[[\"unspliced\"]] &lt;- CreateAssayObject(uf)\n    \n    # Rename cells to avoid barcode collision when merging\n    bundle_row &lt;- grep(str_extract(run_dir, \"([0-9a-z]+-){4}[0-9a-z]+\"), bundle2filename$bundle_uuid)\n    newbarcodes &lt;- paste0(colnames(seu), \"-1-\", rep(bundle2filename[bundle_row, \"file_name\"], length(colnames(seu))))\n    seu &lt;- RenameCells(seu, new.names = newbarcodes)\n    \n    # Merge Seurat objects\n    if (counter == 1) {\n        myesoph &lt;- seu\n    } else {\n        myesoph &lt;- merge(myesoph, seu)\n    }\n    \n    rm(seu)\n}\n\nmyesoph\n\n# Add feature metadata\n# Load an Ensembl gene ID to HGNC symbol `.genes.map.txt` file. \n# All files for spliced and unspliced samples are identical so it doesn't matter which one we load\nensembl2symbol &lt;- read.table(paste0(directories[1], \"/\", \"features.tsv\"), header = FALSE, sep = \"\\t\")\n\n# Rows are left ordered after merges (I checked), so we can add Ensembl IDs directly without more checks\n#myesoph@assays$spliced &lt;- AddMetaData(myesoph@assays$spliced, metadata = ensembl2symbol[[1]], col.name = \"ensembl_id\")\n#myesoph@assays$unspliced &lt;- AddMetaData(myesoph@assays$unspliced, metadata = ensembl2symbol[[1]], col.name = \"ensembl_id\")\n\n# If we want to check before assigning Ensembl IDs to symbols\nmysymbols_match &lt;- match(rownames(myesoph), make.unique(ensembl2symbol[[2]]))\nmyesoph@assays$spliced &lt;- AddMetaData(myesoph@assays$spliced, metadata = ensembl2symbol[[1]][mysymbols_match], col.name = \"ensembl_id\")\nmyesoph@assays$unspliced &lt;- AddMetaData(myesoph@assays$unspliced, metadata = ensembl2symbol[[1]][mysymbols_match], col.name = \"ensembl_id\")\n\n# Save merged Seurat Object\nsaveRDS(myesoph, file = \"~/TFM/2019.12.31_Meyer/results/myesoph_star.rds\")\n\nRun the script\n# [folia]\n\nRscript seurat-analysis_merge.R\nThe R script ended without errors and generated an RDS file.",
    "crumbs": [
      "Trajectory inference analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Joining velocity count matrices in a Seurat Object</span>"
    ]
  },
  {
    "objectID": "combining_seurat_objects.html",
    "href": "combining_seurat_objects.html",
    "title": "8  Combining my Seurat and authors Seurat",
    "section": "",
    "text": "8.1 Comparison of Seurat objects: my object vs paper object\nI compare our analysis with the paper authors one (as requested by Gabriel), with the objective of combining my counts with their object to plot velocity over their UMAP.\nLoad Seurat saved objects into an clean R session.\nhca_esoph.orig &lt;- readRDS(\"~/TFM/2019.12.31_Meyer/results/oesophagus_ts.rds\")\nmyesoph.orig &lt;- readRDS(\"~/TFM/2019.12.31_Meyer/results/myesoph_star.rds\")\n\nhca_esoph &lt;- hca_esoph.orig\nmyesoph &lt;- myesoph.orig",
    "crumbs": [
      "Trajectory inference analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Combining my Seurat and authors Seurat</span>"
    ]
  },
  {
    "objectID": "combining_seurat_objects.html#comparison-of-seurat-objects-my-object-vs-paper-object",
    "href": "combining_seurat_objects.html#comparison-of-seurat-objects-my-object-vs-paper-object",
    "title": "8  Combining my Seurat and authors Seurat",
    "section": "",
    "text": "8.1.1 Comparing barcodes\n\n# Match barcodes between my Seurat object and the paper one\nbcs_common &lt;- intersect(colnames(hca_esoph), colnames(myesoph))\nbcs_no_common &lt;- colnames(hca_esoph)[! colnames(hca_esoph) %in% bcs_common]\n\n# Print the percentage of barcodes retrieved\n#data.frame(\"Number\" = c(length(colnames(hca_esoph)), length(rownames(myesoph))), \n#           row.names = c(\"hca_esoph\", \"myesoph\"))\nc(length(colnames(hca_esoph)), length(colnames(myesoph)))\npc_bcs_match &lt;- (length(bcs_common) / length(colnames(hca_esoph))) * 100\npc_bcs_nomatch &lt;- length(bcs_no_common) / length(colnames(hca_esoph)) * 100\ncat(\"Matched barcodes: \", length(bcs_common), \" / \", length(colnames(hca_esoph)), \" (\", pc_bcs_match, \"%)\\n\", sep = \"\")\ncat(\"Non-matched barcodes: \", length(bcs_no_common), \" / \", length(colnames(hca_esoph)), \" (\", pc_bcs_nomatch, \"%)\\n\", sep = \"\")\n\nWe expect to see a 100% barcode match between the paper’s Seurat object and mine. All of the barcodes in their object should appear in mine, because I basically didn’t filter them. But I get a 77.52% match. The barcodes that appear in the paper object but not in mine come most probably from the 3 samples that I didn’t include in the analysis. I’ll check this.\n\nunique(gsub(\"[ACGT]{16}\", \"\", colnames(hca_esoph)[! colnames(hca_esoph) %in% bcs_common]))\npc_bcs &lt;- (length(bcs_common) / length(grep(\".*74136(20|21|22)\", colnames(hca_esoph), invert = TRUE))) * 100\ncat(\"Matched barcodes:\", pc_bcs, \"%\\n\") \n\nAll of the barcodes that don’t match come from the 3 samples I removed from the analysis for their bad base quality. If I remove these barcodes/cells when calculating the match percentage between my object and the paper’s, I get 100% match (as expected). It’s surprising to see that the authors didn’t remove this samples from the analysis.\n\n# Check barcodes names are identical\nsummary(sort(grep(\".*74136(20|21|22)\", colnames(hca_esoph), invert = TRUE, value = TRUE)) == sort(colnames(myesoph)[colnames(myesoph) %in% bcs_common]))\n\n\n\n8.1.2 Comparing features\n\n# check common features:\ngenes_common &lt;- intersect(rownames(hca_esoph), rownames(myesoph))\ngenes_no_common &lt;- rownames(hca_esoph)[! rownames(hca_esoph) %in% genes_common]\n\nc(length(rownames(hca_esoph)), length(rownames(myesoph)))\npc_genes_match &lt;- length(genes_common) / length(rownames(hca_esoph)) * 100\npc_genes_nomatch &lt;- length(genes_no_common) / length(rownames(hca_esoph)) * 100\ncat(\"Matched genes: \", length(genes_common), \" / \", length(rownames(hca_esoph)), \" (\", pc_genes_match, \"%)\\n\", sep = \"\")\ncat(\"Non-matched genes: \", length(genes_no_common), \" / \", length(rownames(hca_esoph)), \" (\", pc_genes_nomatch, \"%)\\n\", sep = \"\")\n\nThere are 27.85% of gene names unmatched between objects. This percentage is quite high, I’m going to use the Ensembl gene IDs stored in the meta.features slot of the assays to see if I can match more.\n\n# Generate a new metadata column without the version numbers from the Ensembl IDs in myesoph assays\nmyesoph@assays$spliced@meta.features[[\"ensembl_id_noversion\"]] &lt;- str_remove(myesoph@assays$spliced@meta.features$ensembl_id, \"\\\\.\\\\d+\")\nmyesoph@assays$unspliced@meta.features[[\"ensembl_id_noversion\"]] &lt;- str_remove(myesoph@assays$unspliced@meta.features$ensembl_id, \"\\\\.\\\\d+\")\n\n# Match my merged object and the paper one\n# I take the first column of the paper object, because all are the same, and the spliced column\n# from my merged object because the spliced and unspliced ensembl IDs are identical\nens_genes_common &lt;- intersect(hca_esoph@assays$RNA@meta.features[[1]], myesoph@assays$spliced@meta.features$ensembl_id_noversion)\nens_genes_no_common &lt;- hca_esoph@assays$RNA@meta.features[[1]][! hca_esoph@assays$RNA@meta.features[[1]] %in% ens_genes_common]\n\nc(length(rownames(hca_esoph)), length(rownames(myesoph)))\npc_ens_genes_match &lt;- length(ens_genes_common) / length(rownames(hca_esoph)) * 100\npc_ens_genes_nomatch &lt;- length(ens_genes_no_common) / length(rownames(hca_esoph)) * 100\ncat(\"Matched genes: \", length(ens_genes_common), \" / \", length(rownames(hca_esoph)), \" (\", pc_ens_genes_match, \"%)\\n\", sep = \"\")\ncat(\"Non-matched genes: \", length(ens_genes_no_common), \" / \", length(rownames(hca_esoph)), \" (\", pc_ens_genes_nomatch, \"%)\\n\", sep = \"\")\n\nUsing Ensembl IDs 98.31% of genes match between objects. There are 410 Ensembl IDs (1.69%) in the paper’s object that don’t appear in my object. I check if some of these genes actually match by symbol but not ID, because of differences between the reference genomes used.\n\n# Get non matched gene symbols\nens_genes_no_common_symbol &lt;- rownames(hca_esoph)[! hca_esoph@assays$RNA@meta.features[[1]] %in% ens_genes_common]\n\n# See if all the non-match Ensembl IDs are included in the non-matched symbols\n# We would expect that all the genes whose Ensembl IDs don't match, also don't match by symbol\nno_common_ens_and_symbol &lt;- intersect(genes_no_common, ens_genes_no_common_symbol)\n\nprint(length(ens_genes_no_common))\nprint(length(no_common_ens_and_symbol))\n\n# Get genes whose Ensembl ID doesn't match but the symbol does\nens_genes_no_common[ens_genes_no_common_symbol %in% genes_common]\nens_genes_no_common_symbol[ens_genes_no_common_symbol %in% genes_common]\n\nThere are 33 genes that match by symbol but not by Ensembl ID. This is probably due to changes and deprecations in Ensembl IDs between the reference genome the paper authors used and the one that I used. Keeping these genes would improve the match between objects in 0.14%. Such a little improvement isn’t worth the effort that would take to add them to the match. I looked if there are any noticeably important genes, and it doesn’t seem so. I’ll skip these genes.\n\n\n8.1.3 Subsetting objects and merging\nSubset the paper’s object and my merged object so that they have the same barcodes (columns) and genes (rows).\n\n# RESTRICT TO COMMON BARCODES\n# Remove the samples that I didn't include from the paper's object\nhca_esoph &lt;- subset(hca_esoph, cells = grep(\".*74136(20|21|22)\", colnames(hca_esoph), invert = TRUE, value = TRUE))\n# Remove the barcodes that don't appear in the paper's object\nmyesoph &lt;- subset(myesoph, cells = bcs_common)\n\n# RESTRICT TO COMMON FEATURES\nhca_esoph &lt;- subset(hca_esoph, features = rownames(hca_esoph)[hca_esoph@assays$RNA@meta.features[[\"gene.ids-HCATisStab7413619\"]] %in% ens_genes_common])\nmyesoph &lt;- subset(myesoph, features = rownames(myesoph)[myesoph@assays$spliced@meta.features$ensembl_id_noversion %in% ens_genes_common])\n\nCheck that the Seurat objects have columns and rows ordered equally, before joining the objects.\n\n# Check that the barcodes are identical and ordered equally\nsummary(colnames(hca_esoph) == colnames(myesoph))\n\n# Check that the features/genes are identical and ordered equally\nsummary(rownames(hca_esoph) == rownames(myesoph))  # this will be FALSE (different HGNC symbols)\nsummary(hca_esoph[[\"RNA\"]]@meta.features[[1]] == myesoph[[\"spliced\"]]@meta.features$ensembl_id_noversion)\n\nBarcodes (columns) are ordered identically. But features (rows) don’t match between objects, as expected. Each one has HGNC symbols, which are not guaranteed to match, and genes are most probably ordered differently. To add my count matrices to the paper’s object I need to match Ensembl IDs and re-order the rows (genes) in my merged object.\nReorder genes and add my object’s assays to the paper’s object.\n\n# Match rows in my merged object with the papers object to reorder them before merging\nhca_myspliced_match &lt;- match(hca_esoph@assays$RNA@meta.features[[1]], myesoph@assays$spliced@meta.features$ensembl_id_noversion)\nhca_myunspliced_match &lt;- match(hca_esoph@assays$RNA@meta.features[[1]], myesoph@assays$unspliced@meta.features$ensembl_id_noversion)\n\n# Check that we will store the genes correctly\nsummary(myesoph[[\"spliced\"]]@meta.features$ensembl_id_noversion[hca_myspliced_match] == hca_esoph@assays$RNA@meta.features[[1]])\nsummary(myesoph[[\"unspliced\"]]@meta.features$ensembl_id_noversion[hca_myunspliced_match] == hca_esoph@assays$RNA@meta.features[[1]])\n\n\n# Add my assays to the papers object\nhca_esoph[[\"spliced\"]] &lt;- CreateAssayObject(myesoph@assays$spliced@counts[hca_myspliced_match, ])\nhca_esoph[[\"unspliced\"]] &lt;- CreateAssayObject(myesoph@assays$unspliced@counts[hca_myunspliced_match, ])\n\nIf you don’t want to use the Ensembl IDs, and match only HGCN symbols, do the following.\n\nhca_esoph[[\"spliced\"]] &lt;- CreateAssayObject(myesoph@assays$spliced@counts)\nhca_esoph[[\"unspliced\"]] &lt;- CreateAssayObject(myesoph@assays$unspliced@counts)\n\nCompare counts from the papers assay with my assays.\n\n\n\n\n\n\nNote\n\n\n\nTo calculate nCounts manually do: Matrix::colSums(seuratobject@assays$name@counts) To calculate nFeature manually do: ``\nIf you try to do Matrix::colSums(seuratobject[[\"name\"]]), it will sum the values in the slot data that is were normalization values are stored, and the results will be all wrong.\n\n\nCompare counts for spliced and unspliced separately.\n\n# Compare total reads per barcode (nCount) for RNA vs. Spliced & Unspliced\npdf(file=\"results/nCount_comparison.pdf\")\naxis_max &lt;- round_any(max(hca_esoph$nCount_RNA, hca_esoph$nCount_spliced), 10, ceiling)\nplot(hca_esoph$nCount_RNA, hca_esoph$nCount_spliced, xlim = c(0, axis_max), ylim = c(0, axis_max))\nlines(c(0, axis_max), c(0, axis_max), col = \"red\", lty = 2)\nplot(hca_esoph$nCount_RNA, hca_esoph$nCount_unspliced, xlim = c(0, axis_max), ylim = c(0, axis_max))\nlines(c(0, axis_max), c(0, axis_max), col = \"red\", lty = 2)\ndev.off()\n\nsummary(hca_esoph$nCount_RNA)\nsummary(hca_esoph$nCount_spliced)\n\nCompare features for spliced and unspliced separately.\n\n# Compare number of genes per barcode (nFeature) for RNA vs Spliced & Unspliced\npdf(file=\"results/nFeature_comparison.pdf\")\naxis_max &lt;- round_any(max(hca_esoph$nFeature_RNA, hca_esoph$nFeature_spliced), 10, ceiling)\nplot(hca_esoph$nFeature_RNA, hca_esoph$nFeature_spliced, xlim = c(0, axis_max), ylim = c(0, axis_max))\nlines(c(0, axis_max), c(0, axis_max), col = \"red\", lty = 2)\nplot(hca_esoph$nFeature_RNA, hca_esoph$nFeature_unspliced, xlim = c(0, axis_max), ylim = c(0, axis_max))\nlines(c(0, axis_max), c(0, axis_max), col = \"red\", lty = 2)\ndev.off()\n\nsummary(hca_esoph$nFeature_RNA)\nsummary(hca_esoph$nFeature_spliced)\n\nCompare counts and features for spliced and unspliced of my object combined against the paper object.\n\n# Compare nCount and nFeature for RNA vs Spliced+Unspliced\npdf(file=\"results/nCount_nFeature_comparison.pdf\")\naxis_max &lt;- round_any(max(hca_esoph$nCount_RNA, hca_esoph$nCount_spliced), 10, ceiling)\nplot(hca_esoph$nCount_RNA, (hca_esoph$nCount_spliced + hca_esoph$nCount_unspliced), xlim = c(0, axis_max), ylim = c(0, axis_max))\nlines(c(0, axis_max), c(0, axis_max), col = \"red\", lty = 2)\ntitle(\"nCount\")\naxis_max &lt;- round_any(max(hca_esoph$nFeature_RNA, hca_esoph$nFeature_spliced), 10, ceiling)\nplot(hca_esoph$nFeature_RNA, (hca_esoph$nFeature_spliced + hca_esoph$nFeature_unspliced), xlim = c(0, axis_max), ylim = c(0, axis_max))\nlines(c(0, axis_max), c(0, axis_max), col = \"red\", lty = 2)\ntitle(\"nFeature\")\ndev.off()\n\nsummary(hca_esoph$nFeature_RNA)\nsummary(hca_esoph$nFeature_spliced)\n\nCalculate difference between my reads and the paper reads.\n\nsummary((hca_esoph$nCount_spliced + hca_esoph$nCount_unspliced) - hca_esoph$nCount_RNA)\nsummary((hca_esoph$nFeature_spliced + hca_esoph$nFeature_unspliced) - hca_esoph$nFeature_RNA)\n\n# Calculate the difference between my object and the paper's in percentage value\n# A result of 0%, would be that both objects are identical, positive values mean my object has more counts\n(( sum(hca_esoph$nCount_spliced + hca_esoph$nCount_unspliced) / sum(hca_esoph$nCount_RNA) ) - 1 ) * 100\n(( sum(hca_esoph$nFeature_spliced + hca_esoph$nFeature_unspliced) / sum(hca_esoph$nFeature_RNA) ) - 1 ) * 100\n\nJudging by the plots comparing the sum of spliced+unspliced against the paper counts and the summary statistics, we conclude that:\n\nThe paper object has more reads per barcode than my object in average (nCount). My object has -11.5% reads less than the paper object. This could be because of the UMI deduplication strategy used, I used the default “1MM All” of STARsolo, but if you use “Exact”, you could recover more reads per barcode. But there are other possible explanations.\nMy object has more genes per barcode than the paper object in average (nFeature). My object has +9.33% genes per barcode more than the paper object. This could be happening because I haven’t done any filtering at all compared to paper object. But again, there could be other explanations.\n\n\n\n8.1.4 Save comparison object\n\nsaveRDS(hca_esoph, file=\"results/hca_myesoph.rds\")",
    "crumbs": [
      "Trajectory inference analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Combining my Seurat and authors Seurat</span>"
    ]
  },
  {
    "objectID": "trajectory_inference.html",
    "href": "trajectory_inference.html",
    "title": "9  Trajectory inference",
    "section": "",
    "text": "9.1 RNA velocity",
    "crumbs": [
      "Trajectory inference analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Trajectory inference</span>"
    ]
  },
  {
    "objectID": "trajectory_inference.html#rna-velocity",
    "href": "trajectory_inference.html#rna-velocity",
    "title": "9  Trajectory inference",
    "section": "",
    "text": "9.1.1 Prepare input for scvelo\nWe are going to use the package scvelo to perform RNA velocity of the data. This is a python package that forms part of the scverse ecosystem for scRNA analysis. In the scverse, 3 types of data structures are used: anndata, mudata and spatialdata. Scvelo uses the anndata, which is used for normal scRNA-seq.\nThere are 2 tutorials that explain the anndata format: this one from the scverse wiki and this one from the anndata wiki.\nIn order to use scvelo, we need to input an anndata object, so we have to transform our combined Seurat Object to h5ad. I’m going to use the package SeuratDisk for this purpose.\n\n\n\n\n\n\nNote\n\n\n\nI have to modify the Seurat Object so that SeuratDisk saves it correctly. Because by default Convert() saves values that we are not interested in. This behaviour is explained in its help page.\n\n• ‘X’ will be filled with ‘scale.data’ if ‘scale.data’ is present; otherwise, it will be filled with ‘data’ • ‘raw.X’ will be filled with ‘data’ if ‘X’ is filled with ‘scale.data’; otherwise, it will be filled with ‘counts’. If ‘counts’ is not present, then ‘raw’ will not be filled\n\nModifications:\n\nRemove scale.data in all assays: seuratobject@assays$name@scale.data &lt;- matrix(nrow = 0, ncol = 0). (Because it includes less genes than the raw counts)\nChange the counts dgCMatrix so that the values are ‘integer’ instead of ‘numeric’ class. (Numeric can store decimal numbers, so the anndata object actually stores floating points instead of integers, and they are slightly different to the integers)\nOverwrite data with the raw counts: seuratobject@assays$name@data &lt;- seuratobject@assays$name@counts. (Normalization will be performed with scvelo)\n\n\n\n\n### CONVERSION INTO h5ad OBJECT (export to scvelo):\n\n# Load library\n# facilitates conversion between h5Seurat and AnnData objects, i.e. interoperability between Seurat and Scanpy\nlibrary(SeuratDisk)\n\n# Update Seurat Object (the paper object is too old for SeuratDisk to convert)\n# This makes sure the object has all the slots it should and correct errors in the structure\nhca_esoph &lt;- UpdateSeuratObject(hca_esoph)\n\n# Make sure the papers assay is the default (it should already be)\nDefaultAssay(hca_esoph) &lt;- \"RNA\"\n\n# CHANGE THE SEURAT OBJECT FOR CORRECT CONVERSION\n#  Remove scale.data and data from the assays\nremove_scaledata &lt;- function(assay) {\n    assay@scale.data &lt;- matrix(nrow = 0, ncol = 0)\n    return(assay)\n}\n\ncounts_to_integer &lt;- function(assay) {\n    assay@counts@x &lt;- as.integer(assay@counts@x)\n    return(assay)\n}\n\nremove_normalization &lt;- function(assay) {\n    assay@data &lt;- assay@counts\n    return(assay)\n}\n\nhca_esoph@assays &lt;- lapply(hca_esoph@assays, remove_scaledata)\nhca_esoph@assays &lt;- lapply(hca_esoph@assays, counts_to_integer)\nhca_esoph@assays &lt;- lapply(hca_esoph@assays, remove_normalization)\n\n# Add a new metadata column so that cell types are stored as strings, and not as numbers in the anndata\nhca_esoph@meta.data$Celltypes_2020_names &lt;- as.character(hca_esoph@meta.data$Celltypes_updated_July_2020)\n\n# Convert Seurat to H5AD\nSaveH5Seurat(hca_esoph, filename = \"results/hca_myesoph.h5Seurat\", overwrite = TRUE)\nConvert(\"results/hca_myesoph.h5Seurat\", dest = \"h5ad\", overwrite = TRUE)\n\n\n\n9.1.2 Using scvelo\nScvelo has 3 approaches to calculate RNA velocity, these are explained in detail here:\n\nsteady-state/deterministic: the original implementation in La Manno et al paper (2018)\nstochastic: a more modern approach by Bergen et al (2020)\ndynamical: the approach proposed by scvelo developers in Bergen et al (2020)\n\nI’m going to use all of the approaches.\n\nimport scanpy as sp\nimport scvelo as scv\n\n# Funtions to quickly save plots\ndef save_stream(adata, file, format=[\"svg\", \"png\"], **kwargs):\n    for extension in format:\n        file_name = file + \".\" + extension\n        scv.pl.velocity_embedding_stream(adata, save = file_name, **kwargs)\n\ndef save_grid(adata, file, format=[\"svg\", \"png\"], **kwargs):\n    for extension in format:\n        file_name = file + \".\" + extension\n        scv.pl.velocity_embedding_grid(adata, save = file_name, **kwargs)  \n\ndef save_embedding(adata, file, format=[\"svg\", \"png\"], **kwargs):\n    for extension in format:\n        file_name = file + \".\" + extension\n        scv.pl.velocity_embedding(adata, save = file_name, **kwargs)\n\n# Read the data\nadata = scv.read(\"results/hca_myesoph.h5ad\")\n\n# Check that that the object has the information we expect\nadata\nadata.layers.keys()  # should include spliced and unspliced\nadata.layers['spliced']  # check presence\nadata.layers['unspliced']  # check presence\n\n# Filter and normalize\nscv.pp.filter_and_normalize(adata, min_shared_counts=20, n_top_genes=2000)\n\n# Calculate cell moments\nscv.pp.moments(adata, n_pcs=30, n_neighbors=30)\n\n# Estimate RNA velocity with stochastic mode\nscv.tl.velocity(adata, mode='stochastic')\nscv.tl.velocity_graph(adata, n_jobs=6)\nscv.tl.velocity_embedding(adata, basis='umap')\n\n# Plot velocity results\nsave_stream(adata, file=\"results/velocity/scvelo_stochastic/hca_esoph_stream\", format=['svg', 'png'], basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin')\n\nsave_grid(adata, file=\"results/velocity/scvelo_stochastic/hca_esoph_grid\", format=['svg', 'png'], basis='umap', color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3)\n\nsave_embedding(adata, file=\"results/velocity/scvelo_stochastic/hca_esoph_velocity\", format=['svg', 'png'], basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3, dpi=120)\n\n# Save object\nadata.write(\"results/velocity/hca_myesoph_stochastic.h5ad\", compression='gzip')\n\n# Estimate RNA velocity with deterministic mode\nscv.tl.velocity(adata, mode='deterministic')\nscv.tl.velocity_graph(adata, n_jobs=6)\nscv.tl.velocity_embedding(adata, basis='umap')\n\n# Plot velocity results\nsave_stream(adata, file=\"results/velocity/scvelo_deterministic/hca_esoph_stream\", format=['svg', 'png'], basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin')\n\nsave_grid(adata, file=\"results/velocity/scvelo_deterministic/hca_esoph_grid\", format=['svg', 'png'], basis='umap', color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3)\n\nsave_embedding(adata, file=\"results/velocity/scvelo_deterministic/hca_esoph_velocity\", format=['svg', 'png'], basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3, dpi=120)\n\n# Save object\nadata.write(\"results/velocity/hca_myesoph_deterministic.h5ad\", compression='gzip')\n\n# Estimate RNA velocity with dynamical mode\nscv.tl.recover_dynamics(adata, n_jobs=4)\nscv.tl.velocity(adata, mode='dynamical')\nscv.tl.velocity_graph(adata, n_jobs=6)\nscv.tl.velocity_embedding(adata, basis='umap')\n\n# Plot velocity results\nsave_stream(adata, file=\"results/velocity/scvelo_dynamical/hca_esoph_stream\", format=['svg', 'png'], basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin')\n\nsave_grid(adata, file=\"results/velocity/scvelo_dynamical/hca_esoph_grid\", format=['svg', 'png'], basis='umap', color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3)\n\nsave_embedding(adata, file=\"results/velocity/scvelo_dynamical/hca_esoph_velocity\", format=['svg', 'png'], basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3, dpi=120)\n\n# Save object\nadata.write(\"results/velocity/hca_myesoph_dynamical.h5ad\", compression='gzip')\n\n\n9.1.2.1 Velocity over epitelial subset\nWe are only interested in the epithelial cells, so I’ll subset only these cells and apply RNA velocity to them exclusively.\n\n# Subset anndata for epithelial cells\nepi = adata[adata.obs[\"Celltypes_2020_names\"].isin(['Epi_basal', 'Epi_suprabasal', 'Epi_stratified', 'Epi_upper'])].copy()\n\n# Filter and normalize\nscv.pp.filter_and_normalize(epi, min_shared_counts=20, n_top_genes=2000)\n\n# Calculate cell moments\nscv.pp.moments(epi, n_pcs=30, n_neighbors=30)\n\n# Re-calculate the embedding (UMAP)\nscv.tl.umap(epi)\n\n# Estimate RNA velocity with stochastic mode\nscv.tl.velocity(epi, mode='stochastic')\nscv.tl.velocity_graph(epi, n_jobs=6)\nscv.tl.velocity_embedding(epi, basis='umap')\n\n# Plot velocity results\nsave_stream(epi, file=\"results/velocity/scvelo_stochastic/hca_esoph_epi_stream\", format=['svg', 'png'], basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin')\n\nsave_grid(epi, file=\"results/velocity/scvelo_stochastic/hca_esoph_epi_grid\", format=['svg', 'png'], basis='umap', color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3)\n\nsave_embedding(epi, file=\"results/velocity/scvelo_stochastic/hca_esoph_epi_velocity\", format=['svg', 'png'], basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3, dpi=120)\n\n# Save object\nadata.write(\"results/velocity/hca_myesoph_epi_stochastic.h5ad\", compression='gzip')\n\n# Estimate RNA velocity with deterministic mode\nscv.tl.velocity(epi, mode='deterministic')\nscv.tl.velocity_graph(epi, n_jobs=6)\nscv.tl.velocity_embedding(epi, basis='umap')\n\n# Plot velocity results\nsave_stream(epi, file=\"results/velocity/scvelo_deterministic/hca_esoph_epi_stream\", format=['svg', 'png'], basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin')\n\nsave_grid(epi, file=\"results/velocity/scvelo_deterministic/hca_esoph_epi_grid\", format=['svg', 'png'], basis='umap', color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3)\n\nsave_embedding(epi, file=\"results/velocity/scvelo_deterministic/hca_esoph_epi_velocity\", format=['svg', 'png'], basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3, dpi=120)\n\n# Save object\nadata.write(\"results/velocity/hca_myesoph_epi_deterministic.h5ad\", compression='gzip')\n\n# Estimate RNA velocity with dynamical mode\nscv.tl.recover_dynamics(epi, n_jobs=4)\nscv.tl.velocity(epi, mode='dynamical')\nscv.tl.velocity_graph(epi, n_jobs=6)\nscv.tl.velocity_embedding(epi, basis='umap')\n\n# Plot velocity results\nsave_stream(epi, file=\"results/velocity/scvelo_dynamical/hca_esoph_epi_stream\", format=['svg', 'png'], basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin')\n\nsave_grid(epi, file=\"results/velocity/scvelo_dynamical/hca_esoph_epi_grid\", format=['svg', 'png'], basis='umap', color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3)\n\nsave_embedding(epi, file=\"results/velocity/scvelo_dynamical/hca_esoph_epi_velocity\", format=['svg', 'png'], basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3, dpi=120)\n\n# Save object\nadata.write(\"results/velocity/hca_myesoph_epi_dynamical.h5ad\", compression='gzip')\n\n\n\n9.1.2.2 Analyzing velocity output\n\nscv.tl.velocity_confidence(adata)\nscv.pl.scatter(adata, basis='umap', color=['velocity_length', 'velocity_confidence'], color_map='coolwarm', perc=[0, 100], save=\"results/velocity/scvelo_stochastic/hca_myesoph_confidence.png\")\n\n\nscv.tl.rank_velocity_genes(adata, groupby='Celltypes_2020_names', min_corr=0.3)\ndf scv.get_df(adata.uns['rank_velocity_genes']['names'])\ndf.to_csv(\"results/velocity/scvelo_stochastic/hca_myesoph_genes_rank.csv\")\n\n\n\n9.1.2.3 Generic script for scvelo\nI create a generalized script to run all of the RNA velocity steps together. This script will allow me to calculate RNA velocities as well as plot embeddings with velocities, plot velocity confidence, and rank genes.\n\n\"\"\"\nDESCRIPTION: perform RNA velocity and save plots\nAUTHOR: Darío González\n\"\"\"\n\n# Imports\nimport argparse\nimport os\nimport scanpy as sp\nimport scvelo as scv\n\n# Parse arguments\nparser = argparse.ArgumentParser(description=\"Perform RNA velocity and save plots\")\n\nparser.add_argument('-m', '--mode', help=\"Scvelo modes to use and save plots for\", action='store', choices=['stochastic', 'deterministic', 'dynamical'], nargs='+', default=['stochastic', 'deterministic', 'dynamical'])\nparser.add_argument('-d', '--directory', help=\"Folder in to which to store the results\", action='store', default=\".\")\nparser.add_argument('-w', '--write_name', help=\"Base name of the plots files to be written\", action='store', required=True)\nparser.add_argument('-f', '--format', help=\"Formats in which to save the plots. One or more of [svg,png,pdf]\", action='store', nargs='+', choices=['svg', 'png', 'pdf'], default=['svg', 'png'])\nparser.add_argument('-a', '--velocity', help=\"Perform velocity\", action='store_true')\nparser.add_argument('-u', '--umap', help=\"Calculate UMAP\", action='store_true')\nparser.add_argument('-c', '--confidence', help=\"Save confidence plots\", action='store_true')\nparser.add_argument('-r', '--rank', help=\"Save table with velocity gene rank per cluster group\", action='store_true')\nparser.add_argument('-s', '--save', help=\"File name to save anndata object with velocity data\", action='store', default=None)\nparser.add_argument('adata', help=\"File with Anndata object. It should have spliced and unspliced layers.\")\n\nargs = parser.parse_args()\n\nprint(args)\n\n# Funtions to quickly save plots\ndef save_stream(adata, file, format=[\"svg\", \"png\"], **kwargs):\n    for extension in format:\n        file_name = file + \".\" + extension\n        scv.pl.velocity_embedding_stream(adata, save = file_name, **kwargs)\n\ndef save_grid(adata, file, format=[\"svg\", \"png\"], **kwargs):\n    for extension in format:\n        file_name = file + \".\" + extension\n        scv.pl.velocity_embedding_grid(adata, save = file_name, **kwargs)  \n\ndef save_embedding(adata, file, format=[\"svg\", \"png\"], **kwargs):\n    for extension in format:\n        file_name = file + \".\" + extension\n        scv.pl.velocity_embedding(adata, save = file_name, **kwargs)\n\n# Read the data\nadata = scv.read(args.adata)\n\nif args.velocity:\n    # Filter and normalize\n    scv.pp.filter_and_normalize(adata, min_shared_counts=20, n_top_genes=2000)\n\n    # Calculate cell moments\n    scv.pp.moments(adata, n_pcs=30, n_neighbors=30)\n\nif args.umap:\n    scv.tl.umap(adata)\n\nfor mode in args.mode:\n    save_folder = args.directory + \"/scvelo_\" + mode + \"/\"\n    os.makedirs(save_folder, exist_ok=True)\n\n    if args.velocity:\n        # Estimate RNA velocity with stochastic mode\n        if mode == 'dynamical':\n            scv.tl.recover_dynamics(epi, n_jobs=4)\n        \n        scv.tl.velocity(adata, mode=mode)\n        scv.tl.velocity_graph(adata, n_jobs=6)\n        scv.tl.velocity_embedding(adata, basis='umap')\n\n        # Plot velocity results\n        save_stream(adata, file=(save_folder + args.write_name + \"_stream\"), format=args.format, basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin')\n\n        save_grid(adata, file=(save_folder + args.write_name + \"_grid\"), format=args.format, basis='umap', color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3)\n\n        save_embedding(adata, file=(save_folder + args.write_name + \"_velocity\"), format=args.format, basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', arrow_length=5, arrow_size=3, dpi=120)\n\n    # Calculate confidence\n    if args.confidence:\n        scv.tl.velocity_confidence(adata)\n        scv.pl.scatter(adata, basis='umap', color=['velocity_length', 'velocity_confidence'], color_map='coolwarm', perc=[0, 100], save=(save_folder + args.write_name + \"_confidence.svg\"))\n        scv.pl.scatter(adata, basis='umap', color=['velocity_length', 'velocity_confidence'], color_map='coolwarm', perc=[0, 100], save=(save_folder + args.write_name + \"_confidence.png\"))\n\n    # Rank genes\n    if args.rank:\n        scv.tl.rank_velocity_genes(adata, groupby='Celltypes_2020_names', min_corr=0.3)\n        df = scv.get_df(adata.uns['rank_velocity_genes']['names'])\n        df.to_csv(save_folder + args.write_name + \"genes_rank.csv\", index=False)\n\n    # Save object\n    if args.save is not None:\n        save_name = args.save + \"_\" + mode + '.h5ad'\n        adata.write(save_name, compression='gzip')\n\nRun RNA velocity for the HCA data.\n\n# [folia]\n\nwd=/home/dario/TFM/2019.12.31_Meyer/singlecell\nresults=${wd}/results/extending_paper_analysis\n\npython exec_scvelo.py \\\n    -m dynamical \\\n    --velocity \\\n    --confidence \\\n    --rank \\\n    -d ${results}/velocity \\\n    -w hca_myesoph \\\n    -s ${results}/velocity/hca_myesoph \\\n    ${results}/objects/hca_myesoph.h5ad |& tee ${wd}/logs/scvelo_dynamical_$(date '+%Y%m%d_%H%M%S').log\n\nSave a subset of the file with only epithelium cells.\n\n# [folia]\n\nimport scanpy as sc\nimport scvelo as scv\nimport os\n\nos.chdir(\"/home/dario/TFM/2019.12.31_Meyer\")\n\nadata = scv.read(\"results/hca_myesoph.h5ad\")\nepi = adata[adata.obs[\"Celltypes_2020_names\"].isin(['Epi_basal', 'Epi_suprabasal', 'Epi_stratified', 'Epi_upper'])].copy()\nepi.write(\"results/hca_myesoph_epi.h5ad\", compression='gzip')\n\nRun RNA velocity for the epithelial cells subset regenerating the UMAP.\n\n# [folia]\n\nconda activate d_scanpy\n\nwd=/home/dario/TFM/2019.12.31_Meyer/singlecell/results/extending_paper_analysis\n\npython exec_scvelo.py \\\n    -m stochastic deterministic dynamical \\\n    -aucr \\\n    -d ${wd}/velocity \\\n    -w hca_myesoph_epi \\\n    -s ${wd}/velocity/hca_myesoph_epi \\\n    ${wd}/objects/hca_myesoph_epi.h5ad |& tee ${wd}/logs/scvelo_epi_$(date '+%Y%m%d_%H%M%S').log\n\nRun RNA velocity for the epithelial cells subset with the old UMAP.\n\n# [folia]\n\nconda activate d_scanpy\n\nwd=/home/dario/TFM/2019.12.31_Meyer/singlecell/results/extending_paper_analysis\n\npython exec_scvelo.py \\\n    -m stochastic deterministic dynamical \\\n    -acr \\\n    -d ${wd}/velocity \\\n    -w hca_myesoph_epi \\\n    -s ${wd}/velocity/hca_myesoph_epi \\\n    ${wd}/objects/hca_myesoph_epi.h5ad |& tee ${wd/%results*/}/logs/scvelo_epi_$(date '+%Y%m%d_%H%M%S').log\n\n\n\n9.1.2.4 Further analyses\nI’m going to analyze further the results for RNA velocity from scvelo.\n\n# [folia]\n\nimport scanpy as sp\nimport scvelo as scv\n\nresults = \"/home/dario/TFM/2019.12.31_Meyer/singlecell/results/extending_paper_analysis/velocity\"\n\n# Read data in\nadata = scv.read(f\"{results}/hca_myesoph_dynamical.h5ad\")\n\n# Velocity length and confidence per cluster\nkeys = ['velocity_length', 'velocity_confidence']\ndf = adata.obs.groupby('Celltypes_2020_names')[keys].mean().T\ndf.to_csv(f\"{results}/hca_myesoph_dynamical_confidence_per_cluster.csv\")\n\n# Cycling progenitors [NON FUNCTIONAL]\nscv.tl.score_genes_cell_cycle(adata)\nscv.pl.scatter(adata, color_gradients=['S_score', 'G2M_score'], smooth=True, perc=[0, 100], size=20, save=f\"{results}/hca_myesoph_dynamical_cycling_progenitors.svg\")\nscv.pl.scatter(adata, color_gradients=['S_score', 'G2M_score'], smooth=True, perc=[0, 100], figsize=(10, 8), save=f\"{results}/hca_myesoph_dynamical_cycling_progenitors.png\")\n\nRe-generate some of the plots because they have bad quality. I import the saved h5ad with velocity information to re-generate the plots with higher quality for the pngs. Sadly, scvelo plots incorrectly the SVGs, the colorbar is displaced over the plotting area. I could try to troubleshoot the SVG generation, but that will take a long time. So, I can only generate PNGs if I want to get results quickly.\n\nimport scanpy as sp\nimport scvelo as scv\nimport matplotlib.pyplot as plt\n\nwd = \"/home/dario/TFM/2019.12.31_Meyer/singlecell/results/extending_paper_analysis/velocity\"\nresults = [f\"{wd}/scvelo_dynamical\", f\"{wd}/scvelo_stochastic\", f\"{wd}/scvelo_deterministic\"]\n\nfor i in results:\n    # Get model name\n    mode = i.split('_')[-1]\n\n    # Read the data for the model\n    adata = scv.read(f\"{i}/hca_esoph_epi_{mode}.h5ad\")\n\n    # Replot confidence and length PNGs with high resolution\n    scv.pl.scatter(adata, basis='umap', color='velocity_confidence', color_map='coolwarm', perc=[0, 100], size=6, alpha=1, sort_order=False, title=f\"Velocity conficence {mode} model\", dpi=300, save=f\"{i}/hca_esoph_epi_{mode}_confidence.png\")\n    scv.pl.scatter(adata, basis='umap', color='velocity_length', color_map='coolwarm', perc=[0, 100], size=6, alpha=1, sort_order=False, dpi=300, title=f\"Velocity length {mode} model\", save=f\"{i}/hca_esoph_epi_{mode}_length.png\")\n    \n    # Replot stream PNG with high resolution\n    scv.pl.velocity_embedding_stream(adata, basis=\"umap\", color=\"Celltypes_2020_names\", palette='tab20', figsize=(9, 7), legend_loc='right margin', title=f\"RNA velocity stream, {mode} model\", dpi=300, save=(f\"{i}/hca_esoph_epi_{mode}_stream.png\"))\n\n    # Cycling progenitors\n    scv.tl.score_genes_cell_cycle(adata)\n    # Plot cycling progenitors\n    scv.pl.scatter(adata, color_gradients=['S_score', 'G2M_score'], edgecolor='gainsboro', linewidths=0.1, colorbar=True, smooth=False, perc=[0, 100], size=6, alpha=1, save=f\"{i}/hca_esoph_epi_{mode}_cycling_progenitors.svg\")\n    scv.pl.scatter(adata, color_gradients=['S_score', 'G2M_score'], edgecolor='gainsboro', linewidths=0.1, colorbar=True, smooth=False, perc=[0, 100], size=6, alpha=1, dpi=300, save=f\"{i}/hca_esoph_epi_{mode}_cycling_progenitors.png\")\n    \n    scv.pl.scatter(adata, basis='umap', color='phase', palette=['silver', 'coral', 'royalblue'], size=6, alpha=0.4, dpi=300, title=f\"Cell phase, {mode} model\", save=f\"{i}/hca_esoph_epi_{mode}_phase.svg\")\n    scv.pl.scatter(adata, basis='umap', color='phase', palette=['silver', 'coral', 'royalblue'], size=6, alpha=0.4, dpi=300, title=f\"Cell phase, {mode} model\", save=f\"{i}/hca_esoph_epi_{mode}_phase.png\")\n\n    # Save the object\n    adata.write(f\"{i}/hca_esoph_epi_{mode}.h5ad\")\n\nfor i in results:\n    # Get model name\n    mode = i.split('_')[-1]\n\n    # Read the data for the model\n    adata = scv.read(f\"{i}/hca_esoph_epi_{mode}.h5ad\")\n\n    # Get ranked genes\n    df = scv.get_df(adata.uns['rank_velocity_genes']['names'])\n    \n    # Plot 5 most influential genes per cluster\n    kwargs = dict(frameon=False, size=10, linewidth=0.5, )\n\n    #fig, axs = plt.subplots(len(df.columns))\n\n    for j, ct in enumerate(df.columns):\n        #plt.subplot(len(df.columns), 1, j+1)\n        #scv.pl.scatter(adata, df[ct][:5], ylabel=str(ct), **kwargs)\n        #scv.pl.scatter(adata, df[ct][:5], ylabel=str(ct), **kwargs, save=f\"{i}/hca_esoph_epi_{mode}_important_genes_{ct}.svg\")\n        scv.pl.scatter(adata, df[ct][:5], ylabel=str(ct), **kwargs, save=f\"{i}/hca_esoph_epi_{mode}_important_genes_{ct}.png\", dpi=300)\n\n    #plt.savefig(f\"{i}/hca_esoph_epi_{mode}_important_genes.png\", dpi=300)\n    #fig.savefig(f\"{i}/hca_esoph_epi_{mode}_important_genes.svg\")\n    #fig.savefig(f\"{i}/hca_esoph_epi_{mode}_important_genes.png\")",
    "crumbs": [
      "Trajectory inference analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Trajectory inference</span>"
    ]
  },
  {
    "objectID": "trajectory_inference.html#trajectory-inference-with-slingshot",
    "href": "trajectory_inference.html#trajectory-inference-with-slingshot",
    "title": "9  Trajectory inference",
    "section": "9.2 Trajectory inference with Slingshot",
    "text": "9.2 Trajectory inference with Slingshot\n\n\n\n\n\n\nImportant\n\n\n\nThis analysis is entirely of data from the original paper, even if I’m using the combined Seurat object. So, it’s as if I was doing the trajectory inference analysis directly to the original paper object.\n\n\n\n\n\n\n\n\nSlingshot information\n\n\n\nLink: https://bioconductor.org/packages/release/bioc/html/slingshot.html\nCitation: https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-4772-0\nDevelopment: https://github.com/kstreet13/slingshot\n\n\n\n\n\n\n\n\nAbout SingleCellExperiment\n\n\n\nSlingshot uses SingleCellExperiment objects. To understand the ‘SingleCellExperiment’ class organization, it is somewhat documented here and here, and you also need to look the class ‘RangedSummarizedExperiment’, that it extends, which is explained here\n\n\nI’m following the instructions in the trajectory inference Slingshot tutorial.\nAs stated in the this article comparing single cell trajectory inference methods, Slingshot should be used when there aren’t going to be disconnected trajectories. Therefore, it is a good thing that I’m only analyzing a subset of epithelial cells.\nTo run trajectory inference with Slingshot you can use the wrapper function slingshot() (which is recommended), or the functions getLineages() and getCurves(). These functions take a matrix or a “SingleCellExperiment” object as input. Hence, I’ll convert my Seurat object to SingleCellExperiment. There is a Seurat tutorial for conversion.\nI load my comparison object in R and transform it to SingleCellExperiment.\n\n# [folia]\n\n# Imports\nlibrary(Seurat)\n\n# Load comparison Seurat object\nhca_myesoph &lt;- readRDS(\"/home/dario/TFM/2019.12.31_Meyer/results/comparison_hca_myresults/hca_myesoph.rds\")\n\n# Subsetting the object only to epithelial cells\nIdents(hca_myesoph) &lt;- hca_myesoph@meta.data$Celltypes_updated_July_2020\nhca_myesoph_epi &lt;- subset(hca_myesoph, idents = c(\"Epi_basal\", \"Epi_suprabasal\", \"Epi_stratified\", \"Epi_upper\"))\n\n# Convert SeuratObject to SingleCellExperiment with Seurat function 'as.SingleCellExperiment()'\nhca_my_epi_sce &lt;- as.SingleCellExperiment(hca_myesoph_epi)\n\n\nlibrary(slingshot)\n\n# Run trajectory inference with slingshot\nhca_my_epi_sce &lt;- slingshot(hca_my_epi_sce, clusterLabels = 'Celltypes_updated_July_2020', reducedDim = 'UMAP')\n\n###############\n# Plot results\n###############\nlibrary(grDevices)\nlibrary(RColorBrewer)\n\n# Plot trajectory (curves how they are called by Slingshot)\ncolors &lt;- colorRampPalette(brewer.pal(11,'Spectral')[-6])(100)\nplotcol &lt;- colors[cut(hca_my_epi_sce$slingPseudotime_1, breaks=100)]\n\npng(\"results/comparison_hca_myresults/velocity/slingshot_trajectory.png\", width=1000, height=1000, units=\"px\")\n#layout(mat = matrix(1:2, ncol=2), width = c(4, 1), height = c(1, 1))\nplot(reducedDims(hca_my_epi_sce)$UMAP, col = plotcol, pch=16, asp = 1)\nlines(SlingshotDataSet(hca_my_epi_sce), lwd=2, col='black')\n#legend_image &lt;- as.raster(colors, ncol=1)\n#plot(x = c(0, 2), y = c(0, 1), type = 'n', axes = FALSE, xlab = '', ylab = '', main = 'Legend')\n#rasterImage(legend_image, 0, 0, 1, 1)\ndev.off()\n\n# Plot lineage structure\npng(\"results/comparison_hca_myresults/velocity/slingshot_lineage.png\", width=1000, height=1000, units=\"px\")\nplot(reducedDims(hca_my_epi_sce)$UMAP, col = brewer.pal(9,'Set1')[as.numeric(droplevels(hca_my_epi_sce$Celltypes_updated_July_2020))], pch=16, asp = 1)\nlines(SlingshotDataSet(hca_my_epi_sce), lwd=2, type = 'lineages', col = 'black')\nlegend(x=\"topright\", legend=c(\"Epi_basal\", \"Epi_stratified\", \"Epi_suprabasal\", \"Epi_upper\"), col=brewer.pal(9,'Set1')[1:4], pch=16, cex=2)\ndev.off()\n\nThe starting and ending clusters can be found in the ‘SlingshotDataSet’ object, inside the slot slingParams as star.clus and end.clus.",
    "crumbs": [
      "Trajectory inference analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Trajectory inference</span>"
    ]
  }
]